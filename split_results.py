#!/usr/bin/env python3
"""
æ‹†åˆ†å¯¦é©—çµæœæª”æ¡ˆçš„è…³æœ¬
å°‡åŒ…å«å¤šå€‹æ¨¡å‹çš„å¯¦é©—çµæœæ‹†åˆ†æˆæ¯å€‹æ¨¡å‹ç¨ç«‹çš„æª”æ¡ˆ
"""

import os
import re
from pathlib import Path


def split_results_file(input_file: str, output_dir: str = None):
    """
    æ‹†åˆ†å¯¦é©—çµæœæª”æ¡ˆ
    
    Args:
        input_file: è¼¸å…¥æª”æ¡ˆè·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ç‚ºè¼¸å…¥æª”æ¡ˆæ‰€åœ¨ç›®éŒ„ï¼‰
    """
    # è®€å–è¼¸å…¥æª”æ¡ˆ
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è§£ææª”æ¡ˆåç¨±
    input_filename = Path(input_file).stem
    if output_dir is None:
        output_dir = Path(input_file).parent
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¾æª”æ¡ˆåç¨±ä¸­æå–æ¨¡å‹åˆ—è¡¨
    # ä¾‹å¦‚ï¼šdataset_size_all_task_type_all_feature_type_all_models_tabpfn_xgboost_catboost_lightgbm_gnn_stages_none_0.05_0.15_0.8.txt
    match = re.search(r'models_([\w_]+)_gnn_stages', input_filename)
    if not match:
        print(f"âŒ ç„¡æ³•å¾æª”æ¡ˆåç¨±ä¸­è§£ææ¨¡å‹åˆ—è¡¨: {input_filename}")
        return
    
    models_str = match.group(1)
    models = models_str.split('_')
    print(f"ğŸ“ æª¢æ¸¬åˆ°æ¨¡å‹: {models}")
    
    # ç‚ºæ¯å€‹æ¨¡å‹å»ºç«‹è³‡æ–™çµæ§‹
    model_data = {model: [] for model in models}
    current_dataset = None
    current_model = None
    dataset_block = []
    
    # é€è¡Œè§£ææª”æ¡ˆ
    lines = content.split('\n')
    
    for line in lines:
        # æª¢æ¸¬ dataset è¡Œ
        if line.startswith('dataset: '):
            # å¦‚æœæœ‰å…ˆå‰çš„ dataset blockï¼Œå…ˆå„²å­˜
            if current_dataset and current_model and dataset_block:
                model_data[current_model].append('\n'.join(dataset_block))
            
            # é–‹å§‹æ–°çš„ dataset
            current_dataset = line
            dataset_block = []
            current_model = None
        
        # æª¢æ¸¬æ¨¡å‹è¡Œ
        elif line.strip().startswith('æ¨¡å‹: '):
            # å„²å­˜å‰ä¸€å€‹æ¨¡å‹çš„è³‡æ–™
            if current_model and dataset_block:
                model_data[current_model].append('\n'.join(dataset_block))
            
            # æå–æ¨¡å‹åç¨±
            model_match = re.search(r'æ¨¡å‹: (\w+)', line)
            if model_match:
                current_model = model_match.group(1)
                dataset_block = [current_dataset, line]
        
        # ç¹¼çºŒç´¯ç©ç•¶å‰æ¨¡å‹çš„è³‡æ–™
        elif current_model and line.strip():
            dataset_block.append(line)
    
    # è™•ç†æœ€å¾Œä¸€å€‹ dataset block
    if current_dataset and current_model and dataset_block:
        model_data[current_model].append('\n'.join(dataset_block))
    
    # ç‚ºæ¯å€‹æ¨¡å‹å¯«å‡ºæª”æ¡ˆ
    base_filename = input_filename.replace(f'models_{models_str}', 'models_{}')
    
    for model in models:
        if not model_data[model]:
            print(f"âš ï¸  æ¨¡å‹ {model} æ²’æœ‰è³‡æ–™")
            continue
        
        output_filename = base_filename.format(model)
        output_path = output_dir / f"{output_filename}.txt"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(model_data[model]))
        
        dataset_count = len(model_data[model])
        print(f"âœ… å·²å»ºç«‹ {output_path.name} (åŒ…å« {dataset_count} å€‹ datasets)")
    
    print(f"\nğŸ‰ æ‹†åˆ†å®Œæˆï¼å…±æ‹†åˆ†ç‚º {len(models)} å€‹æª”æ¡ˆ")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æ‹†åˆ†å¯¦é©—çµæœæª”æ¡ˆç‚ºæ¯å€‹æ¨¡å‹ç¨ç«‹çš„æª”æ¡ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # æ‹†åˆ†å–®ä¸€æª”æ¡ˆ
  python split_results.py summary_results/dataset_size_all_task_type_all_feature_type_all_models_tabpfn_xgboost_catboost_lightgbm_gnn_stages_none_0.05_0.15_0.8.txt
  
  # æŒ‡å®šè¼¸å‡ºç›®éŒ„
  python split_results.py input.txt -o output_dir/
  
  # è™•ç†ç›®éŒ„ä¸­æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„æª”æ¡ˆ
  python split_results.py summary_results/ --pattern "*_models_*"
        """
    )
    
    parser.add_argument(
        'input',
        help='è¼¸å…¥æª”æ¡ˆè·¯å¾‘æˆ–ç›®éŒ„'
    )
    parser.add_argument(
        '-o', '--output-dir',
        help='è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ç‚ºè¼¸å…¥æª”æ¡ˆæ‰€åœ¨ç›®éŒ„ï¼‰'
    )
    parser.add_argument(
        '--pattern',
        default='*_models_*',
        help='ç•¶è¼¸å…¥ç‚ºç›®éŒ„æ™‚ï¼Œç”¨æ–¼åŒ¹é…æª”æ¡ˆçš„æ¨¡å¼ï¼ˆé è¨­: *_models_*ï¼‰'
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    
    if input_path.is_file():
        # è™•ç†å–®ä¸€æª”æ¡ˆ
        print(f"ğŸ“‚ è™•ç†æª”æ¡ˆ: {input_path}")
        split_results_file(str(input_path), args.output_dir)
    
    elif input_path.is_dir():
        # è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„æª”æ¡ˆ
        files = list(input_path.glob(args.pattern))
        if not files:
            print(f"âŒ åœ¨ç›®éŒ„ {input_path} ä¸­æ‰¾ä¸åˆ°ç¬¦åˆæ¨¡å¼ '{args.pattern}' çš„æª”æ¡ˆ")
            return
        
        print(f"ğŸ“‚ æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
        for file in files:
            print(f"\n{'='*60}")
            print(f"è™•ç†: {file.name}")
            print('='*60)
            split_results_file(str(file), args.output_dir)
    
    else:
        print(f"âŒ è·¯å¾‘ä¸å­˜åœ¨: {input_path}")


if __name__ == '__main__':
    main()
