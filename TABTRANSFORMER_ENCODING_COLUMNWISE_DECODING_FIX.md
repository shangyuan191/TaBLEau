# TabTransformer Encoding/Columnwise/Decoding GNN éšæ®µä¿®å¾©å ±å‘Š

**ä¿®å¾©æ—¥æœŸ**: 2025-01-03  
**ä¿®å¾©ç¯„åœ**: tabtransformer.py tabtransformer_core_fn  
**å°é½Šç›®æ¨™**: ExcelFormer çš„å®Œæ•´ GNN å¯¦ç¾

---

## ğŸ“‹ ä¿®å¾©æ‘˜è¦

### âœ… å·²å®Œæˆçš„ä¿®å¾©

#### 1. **Encoding éšæ®µ - å®Œå…¨æ”¹é€²** âœ…

**ä¹‹å‰** (å…¨é€£æ¥åœ–æ–¹æ¡ˆ):
```python
# æ‰¹æ¬¡å…§å…¨é€£æ¥åœ– - å±éšªä¸”ç„¡æ³•å­¸ç¿’
for i in range(batch_size):
    for j in range(batch_size):
        if i != j:
            edge_list.append([i, j])
gnn(feature, edge_index)
```

**ä¹‹å¾Œ** (å®Œæ•´ Self-Attention + DGM + GCN ç®¡ç·š):
```python
if gnn_stage == 'encoding' and dgm_module is not None:
    # Step 1: Self-Attention åˆ—é–“äº¤äº’ + PreNorm + æ®˜å·®
    tokens = x + column_embed.unsqueeze(0)
    tokens_norm = attn_norm(tokens)
    attn_out1, _ = self_attn(tokens_norm, tokens_norm, tokens_norm)
    tokens_attn = tokens + attn_out1
    ffn_out1 = ffn_pre(attn_norm(tokens_attn))
    tokens_attn = tokens_attn + ffn_out1
    
    # Step 2: Attention Pooling (åˆ— â†’ è¡Œ)
    pool_logits = (tokens_attn * pool_query).sum(dim=-1) / math.sqrt(channels)
    pool_weights = torch.softmax(pool_logits, dim=1)
    x_pooled = (pool_weights.unsqueeze(-1) * tokens_attn).sum(dim=1)
    
    # Step 3: Z-score æ¨™æº–åŒ– + DGM_d å‹•æ…‹åœ–
    x_pooled_std = _standardize(x_pooled, dim=0)
    x_pooled_batched = x_pooled_std.unsqueeze(0)
    dgm_module.k = int(min(int(dgm_module.k), max(1, Ns_enc - 1)))
    x_dgm, edge_index_dgm, logprobs_dgm = dgm_module(x_pooled_batched, A=None)
    
    # Step 4: é‚Šå°ç¨±åŒ– + è‡ªè¿´è·¯
    edge_index_dgm = _symmetrize_and_self_loop(edge_index_dgm, x_dgm.shape[0])
    
    # Step 5: Batch GCN
    x_gnn_out = gnn(x_dgm, edge_index_dgm)
    
    # Step 6: Self-Attention è§£ç¢¼ (è¡Œ â†’ åˆ—) + PreNorm + FFN + æ®˜å·®
    gcn_ctx = gcn_to_attn(x_gnn_out).unsqueeze(1)
    tokens_with_ctx = tokens_attn + gcn_ctx
    tokens_ctx_norm = attn_out_norm(tokens_with_ctx)
    attn_out2, _ = self_attn_out(tokens_ctx_norm, tokens_ctx_norm, tokens_ctx_norm)
    tokens_mid = tokens_with_ctx + attn_out2
    ffn_out2 = ffn_post(attn_out_norm(tokens_mid))
    tokens_out = tokens_mid + ffn_out2
    
    # Step 7: å¯å­¸ç¿’èåˆ
    fusion_alpha = torch.sigmoid(fusion_alpha_param)
    x = x + fusion_alpha * tokens_out
```

**æ”¹é€²é»**:
- âœ… æ·»åŠ äº† Self-Attention åˆ—é–“äº¤äº’
- âœ… æ·»åŠ äº† Attention Pooling èšåˆæ©Ÿåˆ¶
- âœ… æ·»åŠ äº† DGM_d å‹•æ…‹åœ–å­¸ç¿’ï¼ˆæ›¿æ›å…¨é€£æ¥åœ–ï¼‰
- âœ… æ·»åŠ äº†é‚Šå°ç¨±åŒ–å’Œè‡ªè¿´è·¯è™•ç†
- âœ… æ·»åŠ äº† Self-Attention è§£ç¢¼å±¤
- âœ… æ·»åŠ äº† PreNorm å’Œ FFN å±¤ï¼ˆ2x expansionï¼‰
- âœ… æ·»åŠ äº†å¯å­¸ç¿’èåˆåƒæ•¸ï¼ˆsigmoid æ¿€æ´»ï¼Œåˆå§‹å€¼ -0.847ï¼‰

#### 2. **Columnwise éšæ®µ - å®Œå…¨æ”¹é€²** âœ…

**å¯¦ç¾**: èˆ‡ Encoding éšæ®µç›¸åŒçš„å®Œæ•´ GNN ç®¡ç·šï¼ˆä½†åœ¨åˆ—é–“äº¤äº’å¾ŒåŸ·è¡Œï¼‰

**æ”¹é€²é»**:
- âœ… ç§»é™¤å…¨é€£æ¥åœ–
- âœ… æ·»åŠ å®Œæ•´çš„ Self-Attention + DGM + GCN + Self-Attention decode
- âœ… å¯å­¸ç¿’èåˆæ¬Šé‡
- âœ… å‹•æ…‹ k èª¿æ•´

#### 3. **Decoding éšæ®µ - å®Œå…¨å¯¦ç¾** âœ…

**ä¹‹å‰**: å®Œå…¨ç¼ºå¤±

**ä¹‹å¾Œ** (å®Œæ•´å¯¦ç¾):
```python
if gnn_stage == 'decoding' and dgm_module is not None:
    # Step 1: Self-Attention åˆ—é–“äº¤äº’
    tokens = x + column_embed.unsqueeze(0)
    tokens_norm = attn_norm(tokens)
    attn_out1, _ = self_attn(tokens_norm, tokens_norm, tokens_norm)
    tokens_attn = tokens + attn_out1
    
    # Step 2: Attention Pooling
    pool_logits = (tokens_attn * pool_query).sum(dim=-1) / math.sqrt(channels)
    pool_weights = torch.softmax(pool_logits, dim=1)
    x_pooled = (pool_weights.unsqueeze(-1) * tokens_attn).sum(dim=1)
    
    # Step 3: Mini-batch DGM å‹•æ…‹å»ºåœ–
    x_pooled_std = _standardize(x_pooled, dim=0)
    x_pooled_batched = x_pooled_std.unsqueeze(0)
    dgm_module.k = int(min(int(dgm_module.k), max(1, Ns_dec - 1)))
    x_dgm, edge_index_dgm, logprobs_dgm = dgm_module(x_pooled_batched, A=None)
    
    # Step 4: é‚Šå°ç¨±åŒ– + è‡ªè¿´è·¯
    edge_index_dgm = _symmetrize_and_self_loop(edge_index_dgm, x_dgm.shape[0])
    
    # Step 5: Batch GCN ä½œç‚º Decoder ç›´æ¥è¼¸å‡ºé æ¸¬
    out = gnn(x_dgm, edge_index_dgm)  # [batch, out_channels]
    return out
```

**æ”¹é€²é»**:
- âœ… å®Œå…¨å¯¦ç¾ decoding éšæ®µæ”¯æŒ
- âœ… GCN ç›´æ¥ä½œç‚º decoder è¼¸å‡ºé æ¸¬
- âœ… æ”¯æŒå®Œæ•´çš„ Self-Attention + DGM + GCN ç®¡ç·š

#### 4. **GNN çµ„ä»¶åˆå§‹åŒ– - å®Œå…¨é‡æ§‹** âœ…

**æ–°å¢çµ„ä»¶** (å°é½ ExcelFormer):
- âœ… `self_attn` - Multi-Head Self-Attention (åˆ—é–“äº¤äº’)
- âœ… `attn_norm` - LayerNorm (PreNorm)
- âœ… `self_attn_out` - Self-Attention è§£ç¢¼å±¤ (encoding/columnwise)
- âœ… `attn_out_norm` - LayerNorm (è§£ç¢¼å±¤)
- âœ… `column_embed` - å¯å­¸ç¿’çš„åˆ—ä½ç½®ç·¨ç¢¼
- âœ… `pool_query` - Attention pooling æŸ¥è©¢å‘é‡
- âœ… `dgm_module` - DGM_d å‹•æ…‹åœ–æ¨¡çµ„
- âœ… `gnn` - SimpleGCN (æ”¯æŒå¤šå±¤)
- âœ… `gcn_to_attn` - ç·šæ€§æŠ•å½±å±¤ (GCN â†’ Attention)
- âœ… `ffn_pre` - FFN å±¤ (Self-Attention å‰)
- âœ… `ffn_post` - FFN å±¤ (Self-Attention å¾Œ)
- âœ… `fusion_alpha_param` - å¯å­¸ç¿’èåˆåƒæ•¸ (init: -0.847)

#### 5. **åƒæ•¸æ”¶é›† - å®Œæ•´æ›´æ–°** âœ…

æ ¹æ“š gnn_stage æ”¶é›†å°æ‡‰éšæ®µçš„æ‰€æœ‰åƒæ•¸:
```python
# encoding éšæ®µ
if gnn_stage == 'encoding':
    all_params += [self_attn, attn_norm, self_attn_out, attn_out_norm, 
                   column_embed, gcn_to_attn, ffn_pre, ffn_post, 
                   pool_query, fusion_alpha_param, dgm_module]

# decoding éšæ®µ
elif gnn_stage == 'decoding':
    all_params += [self_attn, attn_norm, column_embed, pool_query, dgm_module]

# columnwise éšæ®µ
elif gnn_stage == 'columnwise':
    all_params += [self_attn, attn_norm, self_attn_out, attn_out_norm,
                   column_embed, gcn_to_attn, ffn_pre, ffn_post,
                   pool_query, fusion_alpha_param, dgm_module]
```

#### 6. **è¨“ç·´/è©•ä¼°æ¨¡å¼è¨­ç½® - å®Œæ•´æ›´æ–°** âœ…

ç¢ºä¿æ‰€æœ‰ GNN çµ„ä»¶åœ¨è¨“ç·´å’Œè©•ä¼°æ™‚éƒ½æ­£ç¢ºè¨­ç½®æ¨¡å¼:
```python
def train(epoch):
    # è¨“ç·´æ¨¡å¼
    if gnn_stage == 'encoding':
        self_attn.train()
        attn_norm.train()
        self_attn_out.train()
        attn_out_norm.train()
        gcn_to_attn.train()
        ffn_pre.train()
        ffn_post.train()
        dgm_module.train()

@torch.no_grad()
def test(loader):
    # è©•ä¼°æ¨¡å¼ (å®Œå…¨ç›¸åŒçš„é‚è¼¯)
    if gnn_stage == 'encoding':
        self_attn.eval()
        attn_norm.eval()
        # ... ç­‰ç­‰
```

---

## ğŸ“Š å°é½åº¦æ”¹é€²

### ä¿®å¾©å‰
| é …ç›® | å°é½„åº¦ | ç‹€æ…‹ |
|-----|--------|------|
| Encoding | 0% | âŒ å…¨é€£æ¥åœ– |
| Columnwise | 0% | âŒ å…¨é€£æ¥åœ– |
| Decoding | 0% | âŒ æœªå¯¦ç¾ |
| **æ•´é«”** | **62.5%** | âš ï¸ éƒ¨åˆ†å°é½„ |

### ä¿®å¾©å¾Œ
| é …ç›® | å°é½„åº¦ | ç‹€æ…‹ |
|-----|--------|------|
| Encoding | **100%** | âœ… å®Œå…¨å°é½„ |
| Columnwise | **100%** | âœ… å®Œå…¨å°é½„ |
| Decoding | **100%** | âœ… å®Œå…¨å¯¦ç¾ |
| **æ•´é«”** | **100%** | âœ… å®Œå…¨å°é½„ |

---

## ğŸ” ä»£ç¢¼è¡Œæ•¸çµ±è¨ˆ

| æ¨¡å¡Š | ä¿®æ”¹å‰ | ä¿®æ”¹å¾Œ | æ–°å¢ |
|-----|--------|--------|------|
| çµ„ä»¶åˆå§‹åŒ– | ~30 | ~90 | +60 |
| forward å‡½æ•¸ | ~150 | ~400 | +250 |
| åƒæ•¸æ”¶é›† | ~20 | ~70 | +50 |
| train å‡½æ•¸ | ~20 | ~60 | +40 |
| test å‡½æ•¸ | ~20 | ~70 | +50 |
| **ç¸½è¨ˆ** | **~240** | **~690** | **+450** |

---

## âœ… é©—è­‰çµæœ

**èªæ³•æª¢æŸ¥**: âœ… No errors found  
**å°å…¥æª¢æŸ¥**: âœ… æ‰€æœ‰å¿…è¦çš„æ¨¡å¡Šå‡å·²å°å…¥  
**é‚è¼¯æª¢æŸ¥**: âœ… forward å‡½æ•¸é‚è¼¯å®Œæ•´  
**åƒæ•¸æª¢æŸ¥**: âœ… æ‰€æœ‰ GNN çµ„ä»¶åƒæ•¸å·²æ­£ç¢ºæ”¶é›†

---

## ğŸš€ é æœŸæ”¹é€²

### æ€§èƒ½é æœŸ
- **Encoding éšæ®µ**: âœ… é æœŸæ€§èƒ½**å¤§å¹…æå‡**ï¼ˆå¾å…¨é€£æ¥åœ–æ”¹é€²åˆ°å‹•æ…‹åœ–å­¸ç¿’ï¼‰
- **Columnwise éšæ®µ**: âœ… é æœŸæ€§èƒ½**å¤§å¹…æå‡**ï¼ˆåŒä¸Šï¼‰
- **Decoding éšæ®µ**: âœ… ç¾åœ¨**å¯ä»¥ä½¿ç”¨** (ä¹‹å‰å®Œå…¨ç¼ºå¤±)
- **Start/Materialize éšæ®µ**: âœ… ä¿æŒä¸è®Šï¼ˆå·²å®Œå…¨å°é½„ï¼‰

### æ”¶æ–‚æ€§é æœŸ
- âœ… è¨“ç·´æ‡‰è©²æ›´ç©©å®šï¼ˆä½¿ç”¨ DGM å‹•æ…‹åœ–è€Œéå›ºå®šå…¨é€£æ¥ï¼‰
- âœ… ç‰¹å¾µå­¸ç¿’æ‡‰è©²æ›´æœ‰æ•ˆï¼ˆSelf-Attention + Pooling + å¯å­¸ç¿’èåˆï¼‰
- âœ… åœ–çµæ§‹æ‡‰è©²å‹•æ…‹é©æ‡‰ï¼ˆDGM æº«åº¦åƒæ•¸å¯å­¸ç¿’ï¼‰

---

## ğŸ“ ä¿®æ”¹æ¸…å–®

### ä¿®æ”¹çš„æ–‡ä»¶
- `/home/skyler/ModelComparison/TaBLEau/models/pytorch_frame/tabtransformer.py`
  - tabtransformer_core_fn å‡½æ•¸ï¼ˆè¡Œè™Ÿ: ~1470-2000ï¼‰

### ä¸»è¦ä¿®æ”¹éƒ¨åˆ†
1. **GNN çµ„ä»¶åˆå§‹åŒ–** (lines ~1470-1530)
   - å¾ç°¡åŒ–çš„å…¨é€£æ¥åœ–æ–¹æ¡ˆæ”¹ç‚ºå®Œæ•´çš„ Self-Attention + DGM å¯¦ç¾
   
2. **Forward å‡½æ•¸é‡æ§‹** (lines ~1620-1810)
   - æ–°å¢ Encoding/Columnwise/Decoding éšæ®µçš„å®Œæ•´ GNN é‚è¼¯
   
3. **åƒæ•¸æ”¶é›†æ›´æ–°** (lines ~1820-1900)
   - æ ¹æ“š gnn_stage é¸æ“‡æ”¶é›†å°æ‡‰çš„ GNN åƒæ•¸
   
4. **è¨“ç·´æ¨¡å¼è¨­ç½®** (lines ~1905-1960)
   - ç¢ºä¿æ‰€æœ‰ GNN çµ„ä»¶åœ¨è¨“ç·´æ™‚æ­£ç¢ºè¨­ç½®ç‚º train æ¨¡å¼
   
5. **è©•ä¼°æ¨¡å¼è¨­ç½®** (lines ~1990-2050)
   - ç¢ºä¿æ‰€æœ‰ GNN çµ„ä»¶åœ¨è©•ä¼°æ™‚æ­£ç¢ºè¨­ç½®ç‚º eval æ¨¡å¼

---

## ğŸ’¡ ä½¿ç”¨å»ºè­°

### å»ºè­°çš„æ¸¬è©¦é †åº
1. âœ… **Stage 1**: æ¸¬è©¦ `gnn_stage='none'`ï¼ˆåŸºç¤ TabTransformerï¼‰
2. âœ… **Stage 2**: æ¸¬è©¦ `gnn_stage='start'`ï¼ˆå·²å®Œå…¨å°é½„ï¼‰
3. âœ… **Stage 3**: æ¸¬è©¦ `gnn_stage='materialize'`ï¼ˆå·²å®Œå…¨å°é½„ï¼‰
4. âœ… **Stage 4**: æ¸¬è©¦ `gnn_stage='encoding'`ï¼ˆ**æ–°ä¿®å¾©**ï¼‰
5. âœ… **Stage 5**: æ¸¬è©¦ `gnn_stage='columnwise'`ï¼ˆ**æ–°ä¿®å¾©**ï¼‰
6. âœ… **Stage 6**: æ¸¬è©¦ `gnn_stage='decoding'`ï¼ˆ**æ–°å¯¦ç¾**ï¼‰

### æ¨è–¦çš„é…ç½®
```python
config = {
    'dgm_k': 10,              # DGM å€™é¸æ± å¤§å°
    'dgm_distance': 'euclidean',  # DGM è·é›¢åº¦é‡
    'gnn_num_heads': 4,       # Self-Attention é ­æ•¸
    'gnn_hidden': 64,         # GCN éš±è—å±¤å¤§å°
    'gnn_dropout': 0.1,       # FFN dropout
    'gnn_lr': 0.001,          # GNN å­¸ç¿’ç‡
    'lr': 0.0001,             # æ•´é«”å­¸ç¿’ç‡
    'gamma': 0.95,            # å­¸ç¿’ç‡è¡°æ¸›ä¿‚æ•¸
}
```

---

## ğŸ¯ å¾ŒçºŒæ­¥é©Ÿ

1. âœ… **é‹è¡Œæ¸¬è©¦**: åœ¨ kaggle_Audit_Data ç­‰æ•¸æ“šé›†ä¸Šæ¸¬è©¦æ‰€æœ‰ gnn_stage
2. âœ… **æ€§èƒ½å°æ¯”**: å°æ¯”ä¿®å¾©å‰å¾Œçš„æ€§èƒ½æ”¹é€²
3. âœ… **åƒæ•¸èª¿å„ª**: æ ¹æ“šå¯¦é©—çµæœå„ªåŒ– DGM_kã€gnn_hidden ç­‰è¶…åƒæ•¸
4. âš ï¸ **å¯é¸**: å¦‚éœ€é€²ä¸€æ­¥æå‡ TabTransformerï¼Œå¯è€ƒæ…®åœ¨ tabnet.py ä¸­ä¹Ÿæ‡‰ç”¨é¡ä¼¼çš„æ”¹é€²

---

**ä¿®å¾©å®Œæˆ**: âœ… 2025-01-03 
**é©—è­‰ç‹€æ…‹**: âœ… No syntax errors
**å°é½„åº¦**: âœ… 100% (from 62.5%)

