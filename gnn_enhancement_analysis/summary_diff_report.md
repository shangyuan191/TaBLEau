# Summary diffs (generated by difflib)

This report lists unified diffs between each *_gnn_enhancement_summary.md and its .bak backup. Sections with no changes are still shown.

## excelformer_gnn_enhancement_summary.md

Changed lines (approx): 120



```diff

--- excelformer_gnn_enhancement_summary.md.bak

+++ excelformer_gnn_enhancement_summary.md

@@ -22,18 +22,24 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## Category: large_datasets+binclass+numerical (6 datasets)

 (Reference ranks from source)

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 13.00 | Yes (13.00 < 13.17) | Yes (tie) | 0/3 | 0/3 | 1/2 (beats tabgnn only) | 1/2 (beats tabgnn only; t2g-former much better) | No | No |

-| none (few-shot baseline) | 13.17 | — (baseline) | No | 0/3 | 0/3 | 1/2 (beats tabgnn only) | 1/2 | No | No |

-| decoding | 13.17 | No (tie with baseline) | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+| columnwise | 13.00 | Yes | Yes (tie) | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+| none (few-shot baseline) | 13.17 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+| decoding | 13.17 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

 | encoding | 13.83 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

-| start | 16.33 | No | No | 0/3 | 0/3 | 1/2 | 0/2 (does not beat tabgnn full) | No | No |

+| start | 16.33 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

 | materialize | 16.67 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

 

+

 註：在此類別中，表現最好的為全量訓練的 `tabpfn` 與全量訓練的 `t2g-former`；few-shot EXCELFORMER 變體未能擊敗強大的樹模型或全量基線。在少數情況下，它們僅能擊敗較弱的 `tabgnn`（few/full）。

 

 ---

@@ -42,13 +48,19 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 14.00 | No | No | 0/3 | 0/3 | 1/2 (beats tabgnn only) | 0/2 | No | No |

-| none | 12.67 | — | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+| columnwise | 14.00 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+| none | 12.67 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

 | decoding | 18.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | encoding | 13.67 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

 | start | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | materialize | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

+

 註：在此類別中，全量訓練的 EXCELFORMER 表現相對較佳；few-shot 的注入未能超越在此類別中領先的全量基線或樹/gnn/全量 tabpfn 的表現。

 

 ---

@@ -57,13 +69,19 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| decoding | 5.00 | Yes (5.00 < 12.00) | Yes (5.00 < 19.00) | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 (beats t2g only vs full tabgnn) | No | No |

-| start | 8.00 | Yes | Yes | 3/3?* | 3/3* | 0/2 | 0/2 | No | No |

+| decoding | 5.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+| start | 8.00 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

+

 | materialize | 11.00 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| none | 12.00 | — | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+

+| none | 12.00 | No (tie, few-shot strict) | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+

 | columnwise | 17.00 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | encoding | 18.00 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

+

 *Explanation: tree-based few-shot ranks are catboost 9 / xgboost 13 / lightgbm 15 — decoding (5) and start (8) beat all three; materialize and none beat xgboost/lightgbm but not catboost.

 

 註：在這個單一的分類回歸資料集中，某些 few-shot EXCELFORMER 注入（尤其 `decoding`、`start`、`materialize`）確實能明顯擊敗多數參考方法（包含全量訓練的 EXCELFORMER）。

@@ -74,13 +92,19 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| start | 5.70 | Yes (5.70 < 11.00) | Yes (5.70 < 15.90) | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 | No | No |

+| start | 5.70 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

 | materialize | 6.80 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| none | 11.00 | — | Yes | 1/3 (beats lightgbm only) | 3/3 | 0/2 | 1/2 | No | No |

-| encoding | 11.00 | No (tie with none) | Yes | 1/3 | 3/3 | 0/2 | 1/2 | No | No |

-| columnwise | 11.10 | No | Yes | 0/3 (ties/worse) | 3/3 | 0/2 | 1/2 | No | No |

+

+| none | 11.00 | No (tie, few-shot strict) | Yes | 1/3 | 3/3 | 0/2 | 1/2 | No | No |

+

+| encoding | 11.00 | No (tie, few-shot strict) | Yes | 1/3 | 3/3 | 0/2 | 1/2 | No | No |

+

+| columnwise | 11.10 | No | Yes | 1/3 (tie) | 3/3 | 0/2 | 1/2 | No | No |

+

 | decoding | 13.80 | No | Yes | 0/3 | 3/3 | 0/2 | 1/2 | No | No |

 

+

 註：在大型數值型回歸中，`start` 與 `materialize` few-shot 變體表現最佳——它們能擊敗 few-shot 樹模型與全量樹基線，並擊敗兩個 GNN 基線中的較弱者（t2g/former 的優劣順序視 ratio 而定）。然而，它們仍無法超越 tabpfn。

 

 ---

@@ -89,13 +113,19 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| none | 12.71 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| columnwise | 13.14 | Yes (13.14 < 12.71?) NO -> actually 13.14 > 12.71 -> No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+| none | 12.71 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+| columnwise | 13.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | start | 13.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | encoding | 13.79 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | materialize | 14.36 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | decoding | 15.21 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

+

 註：在小型平衡二分類資料集中，EXCELFORMER 的 few-shot 注入未能超越主要基線或樹/GNN 參考模型；全量訓練的 EXCELFORMER 與全量樹模型 / tabpfn 仍佔主導地位。

 

 ---

@@ -104,13 +134,19 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| start | 8.86 | Yes (8.86 < 12.71) | No (8.86 > 8.29) | 0/3 | 1/3 | 1/2 | 1/2 | No | No |

+| start | 8.86 | Yes | No | 1/3 (tie) | 2/3 (tie) | 1/2 | 1/2 | No | No |

+

 | columnwise | 10.86 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 12.71 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 12.71 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+| none | 12.71 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+| encoding | 12.71 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | decoding | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | materialize | 18.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

+

 註：`start` 是少數會在某些情況改善 few-shot baseline 並勝過部分參考設置（例如部分 GNN/樹模型的全量配置）的注入，但它仍無法擊敗全量表現最好的模型。

 

 ---

@@ -119,13 +155,19 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 12.18 | Yes (12.18 < 12.32) | No | 1/3 (beats xgboost few-shot only) | 0/3 | 1/2 (beats tabgnn few-shot only) | 0/2 | No | No |

-| none | 12.32 | — | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+| columnwise | 12.18 | Yes | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+| none | 12.32 | No (tie, few-shot strict) | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

 | start | 13.07 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

 | encoding | 13.18 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

 | materialize | 14.25 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 | decoding | 15.54 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

+

 註：在許多小型數值二分類資料集中，few-shot EXCELFORMER 注入很少擊敗表現最佳的 few-shot 樹模型或 tabpfn；`columnwise` 與 few-shot baseline 偶爾能擊敗最弱的 few-shot 樹模型（例如某些分割下的 xgboost）及較弱的 tabgnn few-shot。

 

 ---

@@ -134,12 +176,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| decoding | 6.67 | Yes (6.67 < 9.33) | Yes (6.67 < 11.50) | 3/3 | 3/3 | 1/2 | 2/2 | Yes (beats tabpfn few-shot 7.50) | Yes (beats tabpfn full 8.50) |

+| decoding | 6.67 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

 | materialize | 7.33 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

-| start | 8.50 | Yes (8.50 < 9.33) | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No (8.50 > 7.50) | Yes (tie) |

-| encoding | 9.33 | — (equal to baseline none=9.33) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| none | 9.33 | — | Yes | 3/3 | 3/3 | 0/2 | 1/2 | No | No |

-| columnwise | 10.67 | No | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+

+| start | 8.50 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes (tie) |

+

+| encoding | 9.33 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+| none | 9.33 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+| columnwise | 10.67 | No | Yes | 3/3 | 3/3 (tie) | 1/2 | 1/2 | No | No |

+

 

 註：在此類別（小型平衡回歸）中，部分 few-shot EXCELFORMER 注入（特別是 `decoding` 與 `materialize`）明顯超越樹模型基線及 tabpfn。

 

@@ -149,12 +197,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 4.80 | Yes (4.80 < 7.40) | Yes (4.80 < 15.20) | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 | No | No |

-| materialize | 5.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| none | 7.40 | — | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

-| decoding | 7.80 | Yes | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

-| start | 7.80 | Yes | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

-| encoding | 8.00 | Yes | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

+| columnwise | 4.80 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

+| materialize | 5.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

+| none | 7.40 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+| decoding | 7.80 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+| start | 7.80 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+| encoding | 8.00 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

 

 註：在小型分類回歸任務中，數個 few-shot EXCELFORMER 注入（尤其 `columnwise`、`materialize` 等）勝過多數參考方法。

 

@@ -167,4 +221,4 @@

 

 ---

 

-檔案由自動化摘要產生（來源：`excelformer_gnn_enhancement.txt`）。下一步可採取相同格式處理其餘 9 個模型檔案，並彙整整體 GNN 注入效果報告。

+檔案由自動化摘要產生（來源：`excelformer_gnn_enhancement.txt`）。下一步可採取相同格式處理其餘 9 個模型檔案，並彙整整體 GNN 注入效果報告。
```



## fttransformer_gnn_enhancement_summary.md

Changed lines (approx): 48



```diff

--- fttransformer_gnn_enhancement_summary.md.bak

+++ fttransformer_gnn_enhancement_summary.md

@@ -15,9 +15,14 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## large_datasets+binclass+numerical (6 datasets)

 

-Key numbers (from ranking table):

+關鍵數值 (from ranking table):

 - few-shot FTTRANSFORMER (none): 13.50

 - full FTTRANSFORMER (none): 7.50

 - few-shot trees (xgboost/catboost/lightgbm): 10.67 / 8.67 / 9.67

@@ -34,13 +39,13 @@

 | columnwise (few) | 13.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | start (few) | 13.50 | No (tie) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在這個分類/large binary numerical 中，FTTransformer 的 few-shot 注入沒有嚴格超越 few-shot baseline，也未擊敗 full baseline 或 tabpfn。

+註： 在這個分類/large binary numerical 中，FTTransformer 的 few-shot 注入沒有嚴格超越 few-shot baseline，也未擊敗 full baseline 或 tabpfn。

 

 ---

 

 ## large_datasets+multiclass+numerical (3 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 15.00

 - full FT (none): 8.67

 - few-shot trees: catboost 8.33, lightgbm 10.33, xgboost 9.67

@@ -58,13 +63,13 @@

 | materialize (few) | 16.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: `columnwise` 是唯一一個在平均排名上比 few-shot baseline 小的 injection，但仍遠不及 full-sample 參考方法。

+註： `columnwise` 是唯一一個在平均排名上比 few-shot baseline 小的 injection，但仍遠不及 full-sample 參考方法。

 

 ---

 

 ## large_datasets+regression+categorical (1 dataset)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 10.00

 - full FT (none): 3.00

 - few-shot trees: catboost 14.00, xgboost 16.00, lightgbm 18.00

@@ -80,13 +85,13 @@

 | none (few) | 10.00 | — | No | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

 | start (few) | 12.00 | No | No | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在這個單一 categorical regression 資料集中，materialize/encoding/decoding 對 few-shot baseline 有明顯改善，且能擊敗所有樹模型中的 few-shot 變體。

+註： 在這個單一 categorical regression 資料集中，materialize/encoding/decoding 對 few-shot baseline 有明顯改善，且能擊敗所有樹模型中的 few-shot 變體。

 

 ---

 

 ## large_datasets+regression+numerical (10 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 8.70

 - full FT (none): 12.80

 - few-shot trees (xgboost/catboost/lightgbm): 12.00 / 13.30 / 13.50

@@ -104,13 +109,13 @@

 | columnwise (few) | 8.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 | none (few) | 8.70 | — | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 large regression (numerical) 上，FT 的 few-shot 注入（尤其 decoding/start/materialize）可穩定擊敗樹模型並勝過 full baseline。

+註： 在 large regression (numerical) 上，FT 的 few-shot 注入（尤其 decoding/start/materialize）可穩定擊敗樹模型並勝過 full baseline。

 

 ---

 

 ## small_datasets+binclass+balanced (14 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 14.64

 - full FT (none): 7.00

 - few-shot trees: xgboost 11.00, catboost 10.43, lightgbm 10.00

@@ -128,13 +133,13 @@

 | none (few) | 14.64 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: small balanced binary 中，few-shot 的 start/materialize 比 few-shot baseline 小幅改進，但仍落後 full-sample baselines與 tabpfn。

+註： small balanced binary 中，few-shot 的 start/materialize 比 few-shot baseline 小幅改進，但仍落後 full-sample baselines與 tabpfn。

 

 ---

 

 ## small_datasets+binclass+categorical (7 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 15.43

 - full FT (none): 9.29

 - few-shot trees: (varied) lightgbm/t2g/xgboost are strong few-shot players

@@ -149,13 +154,13 @@

 | none (few) | 15.43 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 18.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 small categorical binary 中，start/materialize 有時優於 few-shot baseline，但 full baselines 仍佔優。

+註： 在 small categorical binary 中，start/materialize 有時優於 few-shot baseline，但 full baselines 仍佔優。

 

 ---

 

 ## small_datasets+binclass+numerical (28 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 14.39

 - full FT (none): 5.71

 - few-shot trees/GNNs/tabpfn: various (few-shot tabpfn 9.75, etc.)

@@ -169,13 +174,13 @@

 | none (few) | 14.39 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 17.21 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: Few-shot 注入在此類別通常只帶來小幅改動。

+註： Few-shot 注入在此類別通常只帶來小幅改動。

 

 ---

 

 ## small_datasets+regression+balanced (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 4.33

 - full FT (none): 8.83

 - columnwise (few): 3.67, encoding (few): 3.83

@@ -190,13 +195,13 @@

 | start (few) | 9.50 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 | materialize (few) | 10.00 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: columnwise/encoding 在 small regression balanced 表現優異，能擊敗多數 few-shot 參考方法並超越 full FT baseline。

+註： columnwise/encoding 在 small regression balanced 表現優異，能擊敗多數 few-shot 參考方法並超越 full FT baseline。

 

 ---

 

 ## small_datasets+regression+categorical (5 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 4.00

 - encoding (few): 3.80 (best)

 - tabpfn few: 7.20

@@ -214,7 +219,7 @@

 

 ## small_datasets+regression+numerical (36 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot FT (none): 7.36

 - full FT (none): 12.69

 - decoding (few): 4.58 (very strong)

@@ -231,10 +236,9 @@

 

 ---

 

-### Takeaways (FTTRANSFORMER)

+### 要點總結 (FTTRANSFORMER)

 

 - FTTRANSFORMER 的 few-shot GNN 注入在 regression 任務（特別是 large numeric regression 與 small regression 類別）表現最好；`decoding`、`start`、`materialize` 在 large numeric regression 中最有效。

 - 在多數分類任務與 large classification 中，fully-sampled `tabpfn` 與 fully-sampled 樹方法仍然佔優；few-shot 注入很少超越它們。

 

-註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。

-

+註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。
```



## resnet_gnn_enhancement_summary.md

Changed lines (approx): 58



```diff

--- resnet_gnn_enhancement_summary.md.bak

+++ resnet_gnn_enhancement_summary.md

@@ -4,9 +4,14 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## large_datasets+binclass+numerical (6 datasets)

 

-Key numbers (from ranking):

+關鍵數值 (from ranking):

 - few-shot resnet (none): 13.33

 - full resnet (none): 3.67

 - few-shot trees (xgboost/catboost/lightgbm): 11.00 / 9.17 / 10.00

@@ -24,13 +29,13 @@

 | decoding (few) | 14.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | none (few) | 13.33 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 幾個注入（columnwise/encoding）比 few-shot baseline 略好，但都遠不及 full-sample baselines與 tabpfn。

+註： 幾個注入（columnwise/encoding）比 few-shot baseline 略好，但都遠不及 full-sample baselines與 tabpfn。

 

 ---

 

 ## large_datasets+multiclass+numerical (3 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot resnet (none): 13.33

 - full resnet (none): 3.67

 - few-shot trees: catboost 8.67, xgboost 10.33, lightgbm 11.33

@@ -47,13 +52,13 @@

 | start (few) | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | materialize (few) | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: encoding 輕微優於 few-shot baseline，但依然落後於 full-sample 方法。

+註： encoding 輕微優於 few-shot baseline，但依然落後於 full-sample 方法。

 

 ---

 

 ## large_datasets+regression+categorical (1 dataset)

 

-Key numbers:

+關鍵數值：

 - resnet few-shot decoding: 1.00 (best)

 - resnet few-shot materialize: 6.00, start: 7.00, columnwise: 10.00, none: 11.00, encoding: 12.00

 - tabpfn few/full: 2.00 / 3.00

@@ -69,13 +74,13 @@

 | none (few) | 11.00 | — | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | Yes |

 | encoding (few) | 12.00 | No | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: 在這個單一 categorical regression 資料集中，resnet 的 decoding 幾乎無可匹敵，並且超越了 tabpfn 與其它所有競爭者。

+註： 在這個單一 categorical regression 資料集中，resnet 的 decoding 幾乎無可匹敵，並且超越了 tabpfn 與其它所有競爭者。

 

 ---

 

 ## large_datasets+regression+numerical (10 datasets)

 

-Key numbers:

+關鍵數值：

 - resnet few-shot (none): 10.80

 - resnet full (none): 14.60

 - few-shot resnet injections: decoding 5.70, materialize 6.20, start 6.80, columnwise 9.20, encoding 10.70

@@ -92,13 +97,13 @@

 | encoding (few) | 10.70 | Yes | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

 | none (few) | 10.80 | — | No (full is 14.60) | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: resnet 的 few-shot 注入在 large numeric regression 中表現出色（decoding/materialize/start 均能擊敗樹模型與 full baseline）。

+註： resnet 的 few-shot 注入在 large numeric regression 中表現出色（decoding/materialize/start 均能擊敗樹模型與 full baseline）。

 

 ---

 

 ## small_datasets+binclass+balanced (14 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot resnet (none): 15.07

 - full resnet (none): 7.79

 - resnet few-shot injections: start 12.21, materialize 13.29, columnwise 13.71, encoding 13.71, decoding 15.29

@@ -114,13 +119,13 @@

 | decoding (few) | 15.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | none (few) | 15.07 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: few-shot 注入會比 few-shot baseline 小幅改善，但仍明顯落後 full-sample baselines及 top trees。

+註： few-shot 注入會比 few-shot baseline 小幅改善，但仍明顯落後 full-sample baselines及 top trees。

 

 ---

 

 ## small_datasets+binclass+categorical (7 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot resnet (none): 11.71

 - full resnet (none): 9.29

 - resnet few-shot injections: materialize 11.71, none 11.71, columnwise 13.57, start 14.00, encoding 15.29, decoding 15.29

@@ -136,13 +141,13 @@

 | encoding (few) | 15.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 15.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: small categorical binary 上樹方法與 t2g/tabpfn 佔優，resnet 的 few-shot 注入難以超越它們。

+註： small categorical binary 上樹方法與 t2g/tabpfn 佔優，resnet 的 few-shot 注入難以超越它們。

 

 ---

 

 ## small_datasets+binclass+numerical (28 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot resnet (none): 13.43

 - full resnet (none): 5.25

 - resnet few-shot injections: start 13.29, materialize 13.43, none 13.43, decoding 14.11, encoding 14.57, columnwise 15.04

@@ -157,13 +162,13 @@

 | encoding (few) | 14.57 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 15.04 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在多數 small numeric binary 資料集中，few-shot 注入與 few-shot baseline 相近；full-sample 仍獲得優勢。

+註： 在多數 small numeric binary 資料集中，few-shot 注入與 few-shot baseline 相近；full-sample 仍獲得優勢。

 

 ---

 

 ## small_datasets+regression+balanced (6 datasets)

 

-Key numbers:

+關鍵數值：

 - resnet few-shot decoding: 4.33

 - resnet few-shot start: 6.33, none:7.00, encoding:7.17, materialize:8.50, columnwise:10.83

 - full resnet none: 10.50

@@ -178,13 +183,13 @@

 | materialize (few) | 8.50 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 | columnwise (few) | 10.83 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: decoding 在 small regression balanced 中表現最佳，能擊敗 few-shot 與 full baseline，多數樹方法也被超越。

+註： decoding 在 small regression balanced 中表現最佳，能擊敗 few-shot 與 full baseline，多數樹方法也被超越。

 

 ---

 

 ## small_datasets+regression+categorical (5 datasets)

 

-Key numbers:

+關鍵數值：

 - resnet few-shot decoding: 3.00; start 4.60; encoding 7.20; materialize 8.00; none 8.20; columnwise 8.40

 - full resnet none: 12.80

 - tabpfn few/full: 6.00 / 9.20

@@ -198,13 +203,13 @@

 | none (few) | 8.20 | — | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | Yes |

 | columnwise (few) | 8.40 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: decoding/start 在 small categorical regression 中都優於大多數基線。

+註： decoding/start 在 small categorical regression 中都優於大多數基線。

 

 ---

 

 ## small_datasets+regression+numerical (36 datasets)

 

-Key numbers:

+關鍵數值：

 - resnet few-shot decoding: 3.39 (top)

 - resnet few-shot columnwise: 6.69, materialize: 7.44, start:7.58, encoding:8.31, none:8.39

 - full resnet none: 14.75

@@ -219,14 +224,13 @@

 | encoding (few) | 8.31 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | Yes |

 | none (few) | 8.39 | — | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: resnet 在 small regression numerical 中的 few-shot decoding 表現非常突出，能擊敗 tabgnn/tabpfn 及大多數樹方法。

-

----

-

-### Takeaways (RESNET)

+註： resnet 在 small regression numerical 中的 few-shot decoding 表現非常突出，能擊敗 tabgnn/tabpfn 及大多數樹方法。

+

+---

+

+### 要點總結 (RESNET)

 

 - RESNET 的 few-shot GNN 注入在各類 regression 資料集中（特別是 small/large numeric regression 與 categorical regression）能帶來明顯提升，decoding 與 materialize/start 是常見的贏家。

 - 在多數 classification 任務（尤其 large/mid classification）中，fully-sampled 樹方法與 tabpfn 仍然是最強的 baseline，few-shot 注入通常無法超越。

 

-註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。

-

+註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。
```



## scarf_gnn_enhancement_summary.md

Changed lines (approx): 49



```diff

--- scarf_gnn_enhancement_summary.md.bak

+++ scarf_gnn_enhancement_summary.md

@@ -15,6 +15,11 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## large_datasets+binclass+numerical (6 datasets)

 

 Reference averages from the ranking table (key values used):

@@ -34,13 +39,13 @@

 | encoding (few) | 15.83 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | start (few) | 16.83 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 large_datasets 的 numerical binary 分類組，SCARF 的任何 few-shot GNN 注入都沒有嚴格超越同樣 few-shot 的非 GNN baseline；但全部都優於 full-sample SCARF baseline（因為 full baseline排名特別差）。對 trees、tabpfn、與參考 GNN 都沒有擊敗紀錄。

+註： 在 large_datasets 的 numerical binary 分類組，SCARF 的任何 few-shot GNN 注入都沒有嚴格超越同樣 few-shot 的非 GNN baseline；但全部都優於 full-sample SCARF baseline（因為 full baseline排名特別差）。對 trees、tabpfn、與參考 GNN 都沒有擊敗紀錄。

 

 ---

 

 ## large_datasets+multiclass+numerical (3 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 15.33

 - full SCARF (none): 16.00

 - trees (few): catboost 7.67, lightgbm 9.67, xgboost 9.00

@@ -52,18 +57,18 @@

 | Injection | avg_rank | beats few-shot SCARF? | beats full SCARF? | beats few-shot trees | beats full trees | beats few-shot GNNs | beats full GNNs | beats few tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

 | encoding (few) | 14.67 | Yes (14.67 < 15.33) | Yes (14.67 < 16.00) | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize (few) | 15.33 | No (tie with few-shot) | Yes (15.33 < 16.00) | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+| materialize (few) | 15.33 | No (tie, few-shot strict) | Yes (15.33 < 16.00) | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 16.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | start (few) | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 17.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 只有 encoding 注入在多類別 numerical 上能夠嚴格勝過同樣 few-shot 的 SCARF baseline；materialize 與 full baseline間有優勢但與 trees/其他參考模型相比仍落後。

+註： 只有 encoding 注入在多類別 numerical 上能夠嚴格勝過同樣 few-shot 的 SCARF baseline；materialize 與 full baseline間有優勢但與 trees/其他參考模型相比仍落後。

 

 ---

 

 ## large_datasets+regression+categorical (1 dataset)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 11.00

 - full SCARF (none): 10.00

 - trees (few): catboost 14.00, lightgbm 18.00, xgboost 16.00

@@ -80,13 +85,13 @@

 | start (few) | 8.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 | columnwise (few) | 9.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 

-Notes: 在這個 categorical regression 的單一資料集中，所有 SCARF 的 few-shot GNN 注入均顯著改善且能擊敗樹模型與部分 GNN（比 t2g 好但落後 tabgnn）。tabpfn 遙遙領先。

+註： 在這個 categorical regression 的單一資料集中，所有 SCARF 的 few-shot GNN 注入均顯著改善且能擊敗樹模型與部分 GNN（比 t2g 好但落後 tabgnn）。tabpfn 遙遙領先。

 

 ---

 

 ## large_datasets+regression+numerical (10 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 8.50

 - full SCARF (none): 9.50

 - trees (few): xgboost 12.80, catboost 14.10, lightgbm 14.30

@@ -103,13 +108,13 @@

 | materialize (few) | 8.10 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 | start (few) | 8.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 

-Notes: 在大型 regression (numerical) 上，SCARF 的所有 few-shot 注入均比 few-shot 與 full baseline 更好，並且能一致擊敗三個樹模型（few & full），但仍無法超越 tabpfn / tabgnn 的最頂端表現（只在對比 t2g 上有優勢）。

+註： 在大型 regression (numerical) 上，SCARF 的所有 few-shot 注入均比 few-shot 與 full baseline 更好，並且能一致擊敗三個樹模型（few & full），但仍無法超越 tabpfn / tabgnn 的最頂端表現（只在對比 t2g 上有優勢）。

 

 ---

 

 ## small_datasets+binclass+balanced (14 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 11.79

 - full SCARF (none): 7.14 (note: in this category SCARF full is better than some few-shot variants)

 - trees (few): catboost 10.93, lightgbm 11.29, xgboost 12.50

@@ -127,13 +132,13 @@

 | encoding (few) | 14.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 17.43 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: small balanced binary 的 many-dataset pool 中，SCARF few-shot 變體普遍弱於 full SCARF 與多數基準（tabpfn/trees）。因此 few-shot 注入基本沒有擊敗樹或 GNN 的紀錄。

+註： small balanced binary 的 many-dataset pool 中，SCARF few-shot 變體普遍弱於 full SCARF 與多數基準（tabpfn/trees）。因此 few-shot 注入基本沒有擊敗樹或 GNN 的紀錄。

 

 ---

 

 ## small_datasets+binclass+categorical (7 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 9.14

 - full SCARF (none): 6.14

 - trees (few): lightgbm 6.71, t2g 6.86, tabpfn full 7.57 (some variation)

@@ -150,13 +155,13 @@

 | encoding (few) | 15.43 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 18.57 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: categorical small binary 中 SCARF 的 few-shot 注入未觀察到實質擊敗基準的情形。

+註： categorical small binary 中 SCARF 的 few-shot 注入未觀察到實質擊敗基準的情形。

 

 ---

 

 ## small_datasets+binclass+numerical (28 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 11.93

 - full SCARF (none): 6.89

 - trees (few): catboost 10.39, lightgbm 11.29, xgboost 12.57

@@ -175,13 +180,13 @@

 | columnwise (few) | 14.96 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 16.57 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在大量 small numerical datasets 上 SCARF full baseline 比 few-shot 更具競爭力；few-shot 注入沒有擊敗樹/GNN/tabpfn 的證據。

+註： 在大量 small numerical datasets 上 SCARF full baseline 比 few-shot 更具競爭力；few-shot 注入沒有擊敗樹/GNN/tabpfn 的證據。

 

 ---

 

 ## small_datasets+regression+balanced (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 12.83

 - full SCARF (none): 3.17

 - GNN few: tabgnn (few) 3.00

@@ -195,13 +200,13 @@

 | encoding (few) | 12.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | materialize (few) | 13.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: few-shot decoding 優於 few-shot baseline，但仍落後 full SCARF 及頂尖 GNN（tabgnn）。

+註： few-shot decoding 優於 few-shot baseline，但仍落後 full SCARF 及頂尖 GNN（tabgnn）。

 

 ---

 

 ## small_datasets+regression+categorical (5 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 14.40

 - full SCARF (none): 9.20

 - tabgnn few: 2.60 ; tabpfn few: 3.40

@@ -214,13 +219,13 @@

 | encoding (few) | 12.20 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 13.20 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: decoding 在 small regression categorical 上效果最好，能擊敗多數 baseline 與 trees，但仍被 tabpfn/tabgnn 頂掉。

+註： decoding 在 small regression categorical 上效果最好，能擊敗多數 baseline 與 trees，但仍被 tabpfn/tabgnn 頂掉。

 

 ---

 

 ## small_datasets+regression+numerical (36 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SCARF (none): 12.03

 - full SCARF (none): 9.58

 - top few-shot competitors: tabgnn (few) 3.42, tabpfn (few) 3.69

@@ -234,7 +239,7 @@

 | materialize (few) | 11.69 | Yes | No (11.69 > 9.58) | 1/3 | 1/3 | 0/2 | 0/2 | No | No |

 | start (few) | 12.06 | No (12.06 > 12.03) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在大量 small regression datasets 中，decoding 與 columnwise/encoding 在某些情形下能顯著超過 few/full baseline 並擊敗多數樹模型，但 tabpfn/tabgnn 仍居前列。

+註： 在大量 small regression datasets 中，decoding 與 columnwise/encoding 在某些情形下能顯著超過 few/full baseline 並擊敗多數樹模型，但 tabpfn/tabgnn 仍居前列。

 

 ---

 

@@ -246,4 +251,4 @@

 - overall：SCARF 的 few-shot GNN 注入在 regression 類別（尤其 numerical）更容易產生可觀改進；在分類任務上較少見顯著勝出。

 

 

-*註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。
+*註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。
```



## subtab_gnn_enhancement_summary.md

Changed lines (approx): 49



```diff

--- subtab_gnn_enhancement_summary.md.bak

+++ subtab_gnn_enhancement_summary.md

@@ -15,9 +15,14 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## large_datasets+binclass+numerical (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 15.50

 - full SUBTAB (none): 12.33

 - trees (few): catboost 7.67, lightgbm 8.33, xgboost 9.17

@@ -35,13 +40,13 @@

 | materialize (few) | 18.17 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | start (few) | 18.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 large binary numerical 中，subtab 的 decoding( few ) 是唯一一個比 few-shot baseline 更好的注入，但仍落後 full-sample baseline。

+註： 在 large binary numerical 中，subtab 的 decoding( few ) 是唯一一個比 few-shot baseline 更好的注入，但仍落後 full-sample baseline。

 

 ---

 

 ## large_datasets+multiclass+numerical (3 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 14.33

 - full SUBTAB (none): 12.00

 - trees (few): catboost 7.67, lightgbm 9.67, xgboost 9.00

@@ -59,13 +64,13 @@

 | start (few) | 17.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 19.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 沒有 few-shot 注入能擊敗 full baseline 或參考模型。

+註： 沒有 few-shot 注入能擊敗 full baseline 或參考模型。

 

 ---

 

 ## large_datasets+regression+categorical (1 dataset)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 9.00

 - full SUBTAB (none): 11.00

 - trees (few): catboost 14.00, lightgbm 18.00, xgboost 16.00

@@ -80,13 +85,13 @@

 | encoding (few) | 8.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 | none (few) | 9.00 | — | — | — | — | — | — | — | — |

 

-Notes: 在這個類別 subtab 的很多 few-shot 注入均能擊敗多數比較對象。

+註： 在這個類別 subtab 的很多 few-shot 注入均能擊敗多數比較對象。

 

 ---

 

 ## large_datasets+regression+numerical (10 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 7.60

 - full SUBTAB (none): 9.20

 - trees (few): xgboost 12.80, catboost 14.10, lightgbm 14.30

@@ -102,13 +107,13 @@

 | columnwise (few) | 7.90 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 | decoding (few) | 8.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

 

-Notes: 對於 large regression numerical，subtab 幾乎所有 few-shot 注入均比 few/full baseline 好，並且能擊敗三個樹模型。

+註： 對於 large regression numerical，subtab 幾乎所有 few-shot 注入均比 few/full baseline 好，並且能擊敗三個樹模型。

 

 ---

 

 ## small_datasets+binclass+balanced (14 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 11.57

 - full SUBTAB (none): 8.71

 - tabpfn few: 8.00 ; tabpfn full: 5.29

@@ -124,13 +129,13 @@

 | decoding (few) | 13.75 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | materialize (few) | 13.96 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: small balanced binary 中 subtab 的 few-shot 注入多數未擊敗 full baseline 或 tabpfn。

+註： small balanced binary 中 subtab 的 few-shot 注入多數未擊敗 full baseline 或 tabpfn。

 

 ---

 

 ## small_datasets+binclass+categorical (7 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 14.57

 - full SUBTAB (none): 8.71

 - t2g/full and tabpfn values near top

@@ -144,13 +149,13 @@

 | columnwise (few) | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在這個子集，少數（start）注入能稍微優於 few-shot baseline。

+註： 在這個子集，少數（start）注入能稍微優於 few-shot baseline。

 

 ---

 

 ## small_datasets+binclass+numerical (28 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 11.54

 - full SUBTAB (none): 6.82

 - tabpfn few: 10.57 ; tabpfn full: 3.21

@@ -166,13 +171,13 @@

 | decoding (few) | 13.75 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | materialize (few) | 13.96 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在這個 28 個 small numeric dataset 的集合中 few-shot 注入未見優勢。

+註： 在這個 28 個 small numeric dataset 的集合中 few-shot 注入未見優勢。

 

 ---

 

 ## small_datasets+regression+balanced (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 4.00

 - top few-shot subtab variants: materialize 3.50, columnwise 3.83, encoding 3.83

 - tabpfn few: 10.17 ; tabpfn full: 11.00

@@ -186,13 +191,13 @@

 | start (few) | 4.50 | No | Yes | 2/3 | 2/3 | 1/2 | 1/2 | Yes | Yes |

 | decoding (few) | 11.17 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: subtab 在 small regression balanced 表現出強烈的 few-shot GNN 改善，materialize/columnwise/encoding 特別有效。

+註： subtab 在 small regression balanced 表現出強烈的 few-shot GNN 改善，materialize/columnwise/encoding 特別有效。

 

 ---

 

 ## small_datasets+regression+categorical (5 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 6.80

 - full SUBTAB (none): 9.60

 - tabpfn few: 7.60 ; tabpfn full: 10.00

@@ -206,13 +211,13 @@

 | none (few) | 6.80 | — | — | — | — | — | — | — | — |

 | decoding (few) | 10.80 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 很明顯 subtab 在 small regression categorical 的 few-shot GNN 變體帶來巨大提升。

+註： 很明顯 subtab 在 small regression categorical 的 few-shot GNN 變體帶來巨大提升。

 

 ---

 

 ## small_datasets+regression+numerical (36 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot SUBTAB (none): 4.89

 - full SUBTAB (none): 10.06

 - top few-shot: start 4.31, materialize 4.61, columnwise 4.72, encoding 4.89

@@ -227,7 +232,7 @@

 | none (few) | 4.89 | — | — | — | — | — | — | — | — |

 | decoding (few) | 10.03 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: subtab 在 small regression numerical 展示強大的 few-shot 增益，幾乎所有 GNN 注入都優於基線並擊敗樹/GNN/tabpfn 的多數對手。

+註： subtab 在 small regression numerical 展示強大的 few-shot 增益，幾乎所有 GNN 注入都優於基線並擊敗樹/GNN/tabpfn 的多數對手。

 

 ---

 

@@ -239,4 +244,4 @@

 - 在分類任務（尤其 small-dataset binary/categorical），few-shot 注入的表現較混合，但仍可見少數場景（例如 start 注入）帶來小幅提升。

 

 

-*註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。
+*註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。
```



## tabm_gnn_enhancement_summary.md

Changed lines (approx): 47



```diff

--- tabm_gnn_enhancement_summary.md.bak

+++ tabm_gnn_enhancement_summary.md

@@ -15,9 +15,14 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## large_datasets+binclass+numerical (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 12.67

 - full TABM (none): 12.00

 - trees (few): catboost 7.67, lightgbm 8.33, xgboost 9.17

@@ -36,13 +41,13 @@

 | start (few) | 17.17 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 17.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 large binary numerical 中，TABM 的 few-shot 注入通常未提供明顯優勢。

+註： 在 large binary numerical 中，TABM 的 few-shot 注入通常未提供明顯優勢。

 

 ---

 

 ## large_datasets+multiclass+numerical (3 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 10.00

 - full TABM (none): 2.67

 - trees (few): catboost 9.00, lightgbm 11.33, xgboost 10.67

@@ -61,13 +66,13 @@

 | start (few) | 16.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 19.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: few-shot 注入在多類別 large datasets 沒有帶來明顯改善。

+註： few-shot 注入在多類別 large datasets 沒有帶來明顯改善。

 

 ---

 

 ## large_datasets+regression+categorical (1 dataset)

 

-Key numbers:

+關鍵數值：

 - best: tabm encoding (few) 1.00

 - few-shot TABM (none): 11.00

 - full TABM (none): 15.00

@@ -82,13 +87,13 @@

 | decoding (few) | 12.00 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | none (few) | 11.00 | — | — | — | — | — | — | — | — |

 

-Notes: 對於這個 categorical regression 的單一 dataset，tabm 的 encoding few-shot 變體表現最佳且能擊敗所有比較對象。

+註： 對於這個 categorical regression 的單一 dataset，tabm 的 encoding few-shot 變體表現最佳且能擊敗所有比較對象。

 

 ---

 

 ## large_datasets+regression+numerical (10 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 12.60

 - full TABM (none): 17.20

 - trees (few): xgboost 10.50, catboost 12.10, lightgbm 12.50

@@ -104,13 +109,13 @@

 | encoding (few) | 8.90 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

 | none (few) | 12.60 | — | — | — | — | — | — | — | — |

 

-Notes: 在 large regression numerical，TABM 的多個 few-shot 注入都能顯著擊敗 few-與 full-baseline 以及樹模型。

+註： 在 large regression numerical，TABM 的多個 few-shot 注入都能顯著擊敗 few-與 full-baseline 以及樹模型。

 

 ---

 

 ## small_datasets+binclass+balanced (14 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 11.00

 - full TABM (none): 11.57

 - tabpfn few: 7.79 ; tabpfn full: 5.21

@@ -126,13 +131,13 @@

 | encoding (few) | 13.57 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 14.57 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 small balanced binary，tabm 的 few-shot 變體沒有明顯帶來改進。

+註： 在 small balanced binary，tabm 的 few-shot 變體沒有明顯帶來改進。

 

 ---

 

 ## small_datasets+binclass+categorical (7 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 10.29

 - full TABM (none): 9.57

 - tabpfn few: 8.71 ; tabpfn full: 7.86

@@ -147,13 +152,13 @@

 | decoding (few) | 14.43 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | none (few) | 10.29 | — | — | — | — | — | — | — | — |

 

-Notes: few-shot 在 small categorical binary 中沒有明顯優勢。

+註： few-shot 在 small categorical binary 中沒有明顯優勢。

 

 ---

 

 ## small_datasets+binclass+numerical (28 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 12.00

 - full TABM (none): 9.39

 - tabpfn few: 9.68 ; tabpfn full: 3.07

@@ -175,7 +180,7 @@

 

 ## small_datasets+regression+balanced (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 9.33

 - full TABM (none): 12.33

 - top few-shot: columnwise 6.83, encoding 7.33, decoding 8.17

@@ -190,13 +195,13 @@

 | start (few) | 10.17 | No | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

 | materialize (few) | 9.67 | No | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: 在 small regression balanced，部分 few-shot 注入能顯著改進。

+註： 在 small regression balanced，部分 few-shot 注入能顯著改進。

 

 ---

 

 ## small_datasets+regression+categorical (5 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 9.20

 - full TABM (none): 9.60

 - top: tabgnn few 3.20 ; tabpfn few 4.80

@@ -210,13 +215,13 @@

 | none (few) | 9.20 | — | — | — | — | — | — | — | — |

 | decoding (few) | 10.80 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: tabm 在 small categorical regression 的 few-shot encoding/columnwise 有明顯提升。

+註： tabm 在 small categorical regression 的 few-shot encoding/columnwise 有明顯提升。

 

 ---

 

 ## small_datasets+regression+numerical (36 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot TABM (none): 9.42

 - full TABM (none): 15.17

 - top few-shot: columnwise 5.28, encoding 5.75, decoding 8.06, materialize 8.42, start 9.28

@@ -231,7 +236,7 @@

 | start (few) | 9.28 | Yes | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

 | none (few) | 9.42 | — | — | — | — | — | — | — | — |

 

-Notes: 在 small regression numerical 上 tabm 多個 few-shot 注入都帶來實質改善，尤其 columnwise/encoding。

+註： 在 small regression numerical 上 tabm 多個 few-shot 注入都帶來實質改善，尤其 columnwise/encoding。

 

 ---

 

@@ -243,4 +248,4 @@

 - 個別資料集（例如 large regression categorical）中，tabm 的某些 few-shot encoding 變體可達到頂尖表現。

 

 

-*註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。
+*註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。
```



## tabnet_gnn_enhancement_summary.md

Changed lines (approx): 58



```diff

--- tabnet_gnn_enhancement_summary.md.bak

+++ tabnet_gnn_enhancement_summary.md

@@ -4,9 +4,14 @@

 

 ---

 

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

+

 ## large_datasets+binclass+numerical (6 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot tabnet (none): 13.17

 - full tabnet (none): 5.17

 - few-shot trees: catboost 8.50, lightgbm 9.17, xgboost 10.00

@@ -23,13 +28,13 @@

 | columnwise (few) | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | encoding (few) | 18.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 large binary numerical 中，TabNet 的 few-shot 注入與 few-shot baseline 相差不大；full-sample 和 tabpfn 仍佔優。

+註： 在 large binary numerical 中，TabNet 的 few-shot 注入與 few-shot baseline 相差不大；full-sample 和 tabpfn 仍佔優。

 

 ---

 

 ## large_datasets+multiclass+numerical (3 datasets)

 

-Key numbers:

+關鍵數值：

 - few-shot tabnet (none): 14.00

 - full tabnet (none): 5.67

 - few-shot trees/GNNs: catboost few 8.67, t2g few 8.67, tabpfn few 8.67

@@ -45,13 +50,13 @@

 | encoding (few) | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 19.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在這個 multiclass small set 中，few-shot 注入未見改善。

+註： 在這個 multiclass small set 中，few-shot 注入未見改善。

 

 ---

 

 ## large_datasets+regression+categorical (1 dataset)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot encoding: 1.00 (best)

 - tabpfn few/full: 2.00 / 4.00

 - tabnet few columnwise: 3.00, materialize: 7.00, start: 10.00, none: 11.00, decoding: 12.00

@@ -66,13 +71,13 @@

 | none (few) | 11.00 | — | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 | decoding (few) | 12.00 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: encoding 是此 dataset 的最佳 few-shot 注入，甚至擊敗 tabpfn。

+註： encoding 是此 dataset 的最佳 few-shot 注入，甚至擊敗 tabpfn。

 

 ---

 

 ## large_datasets+regression+numerical (10 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot encoding: 2.00

 - tabpfn few/full: 2.80 / 2.60

 - tabnet few columnwise: 3.00, start/materialize: 7.80, none 10.20, decoding 16.50

@@ -87,13 +92,13 @@

 | none (few) | 10.20 | — | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

 | decoding (few) | 16.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: tabnet 在 large numeric regression 的 few-shot encoding/columnwise 表現很好，能擊敗多數 few-shot 樹方法，encoding 甚至超越 tabpfn。

+註： tabnet 在 large numeric regression 的 few-shot encoding/columnwise 表現很好，能擊敗多數 few-shot 樹方法，encoding 甚至超越 tabpfn。

 

 ---

 

 ## small_datasets+binclass+balanced (14 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot none: 12.79

 - tabnet few-shot decoding: 11.36, start 11.93, materialize 13.29, columnwise 17.29, encoding 17.43

 - full tabnet none: 7.50

@@ -109,13 +114,13 @@

 | columnwise (few) | 17.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | encoding (few) | 17.43 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: small balanced binary 中 few-shot decoding/start 能小幅超越 few-shot baseline，但仍落後於 full-sample 參考。

+註： small balanced binary 中 few-shot decoding/start 能小幅超越 few-shot baseline，但仍落後於 full-sample 參考。

 

 ---

 

 ## small_datasets+binclass+categorical (7 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot none: 11.00

 - top few-shot competitors: t2g few 7.00, lightgbm few 6.00, tabpfn few 7.29

 - tabnet few-shot injections: none 11.00, materialize 13.86, decoding 14.43, start 14.71, encoding 17.71, columnwise 18.86

@@ -129,13 +134,13 @@

 | encoding (few) | 17.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 18.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: small categorical binary 中 TabNet few-shot 注入通常無法超越基線或強樹方法。

+註： small categorical binary 中 TabNet few-shot 注入通常無法超越基線或強樹方法。

 

 ---

 

 ## small_datasets+binclass+numerical (28 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot none: 11.32

 - tabpfn few: 10.14, t2g few: 9.46

 - tabnet few-shot injections: decoding 11.36, materialize 12.71, start 12.75, encoding 17.25, columnwise 17.71

@@ -149,13 +154,13 @@

 | encoding (few) | 17.25 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 | columnwise (few) | 17.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

 

-Notes: 在 small numeric binary 分類中，TabNet 的 few-shot 注入並不總能顯著超越 few-shot baseline 或競爭方法。

+註： 在 small numeric binary 分類中，TabNet 的 few-shot 注入並不總能顯著超越 few-shot baseline 或競爭方法。

 

 ---

 

 ## small_datasets+regression+balanced (6 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot columnwise: 8.83, encoding:7.17, decoding:8.67, materialize:8.67, start:10.33, none:10.83

 - tabpfn few/full: 6.83 / 8.50; tabgnn few: 3.33

 

@@ -168,13 +173,13 @@

 | start (few) | 10.33 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 | none (few) | 10.83 | — | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: 在 small regression balanced 中，encoding 是 few-shot 中較強的注入，能擊敗 full-sample 大多數基線。

+註： 在 small regression balanced 中，encoding 是 few-shot 中較強的注入，能擊敗 full-sample 大多數基線。

 

 ---

 

 ## small_datasets+regression+categorical (5 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot decoding: 4.20, columnwise:6.20, encoding:6.60, materialize:7.40, start:8.80, none:9.40

 - tabpfn few: 4.40, tabgnn few: 3.60

 

@@ -187,13 +192,13 @@

 | start (few) | 8.80 | Yes | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 | none (few) | 9.40 | — | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: decoding 在 small categorical regression 中表現良好且可擊敗 tabpfn/few trees。

+註： decoding 在 small categorical regression 中表現良好且可擊敗 tabpfn/few trees。

 

 ---

 

 ## small_datasets+regression+numerical (36 datasets)

 

-Key numbers:

+關鍵數值：

 - tabnet few-shot columnwise: 5.47, encoding:5.78, decoding:7.44, start:8.33, materialize:8.69, none:8.83

 - tabgnn few: 4.44, tabpfn few: 4.67

 

@@ -206,14 +211,13 @@

 | materialize (few) | 8.69 | Yes | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

 | none (few) | 8.83 | — | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

 

-Notes: TabNet 在 small numeric regression 中的 columnwise/encoding 表現很穩定，勝過大部分樹模型，雖然 tabgnn/tabpfn few-shot 仍更強。

-

----

-

-### Takeaways (TABNET)

+註： TabNet 在 small numeric regression 中的 columnwise/encoding 表現很穩定，勝過大部分樹模型，雖然 tabgnn/tabpfn few-shot 仍更強。

+

+---

+

+### 要點總結 (TABNET)

 

 - TabNet 的 few-shot GNN 注入在 regression 任務（尤其 large numeric regression 與 small regression 類別）有明顯幫助；encoding/columnwise/decoding 通常是最佳注入位置。

 - 在大多數 classification 類別，tabpfn (full) 或 fully-sampled trees 仍領先。

 

-註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。

-

+註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。
```



## tabtransformer_gnn_enhancement_summary.md

Changed lines (approx): 193



```diff

--- tabtransformer_gnn_enhancement_summary.md.bak

+++ tabtransformer_gnn_enhancement_summary.md

@@ -2,11 +2,16 @@

 

 說明：本檔根據 `tabtransformer_gnn_enhancement.md` 的 avg_rank 值，按 dataset-category 匯總 few-shot（ratio=0.05/0.15/0.8）下各種 GNN 注入變體是否擊敗指定的目標組。

 

-註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。

+註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。

 

 考慮的 few-shot TABTRANSFORMER 注入：`columnwise`、`none`（few-shot baseline）、`decoding`、`encoding`、`start`、`materialize`（皆為 ratio=0.05/0.15/0.8）。

 

 ---

+

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

 

 ## Category: large_datasets+binclass+numerical (6 datasets)

 (Reference ranks from source)

@@ -22,12 +27,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 16.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none (few-shot baseline) | 15.00 | — | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

-| decoding | 12.83 | Yes (12.83 < 15.00) | No | 0/3 | 0/3 | 1/2 (beats tabgnn only) | 1/2 (beats tabgnn full only) | No | No |

-| encoding | 15.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 17.17 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 16.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 16.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| none (few-shot baseline) | 15.00 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| decoding | 12.83 | Yes | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 15.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| start | 17.17 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 16.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在此類別中，few-shot TABTRANSFORMER 的 `decoding` 較 few-shot baseline 略有改善，但沒有任何 few-shot 注入能擊敗 full-tabtransformer 或強勢的 full-sample 樹模型與 tabpfn。

 

@@ -47,12 +58,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 14.33 | No (tie) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 14.33 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 17.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 14.00 | Yes (14.00 < 14.33) | No | 0/3 | 0/3 | 1/2 (beats tabgnn only) | 0/2 | No | No |

-| start | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 14.33 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| none | 14.33 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| decoding | 17.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 14.00 | Yes | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| start | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在這個 small multiclass 類別，few-shot 的 `encoding` 對 few-shot baseline 有小幅提升，但仍與 full-sample 的領先者（如 t2g、tabpfn）有差距。

 

@@ -72,12 +89,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 1.00 | Yes | Yes | 3/3 | 3/3 | 1/2 (beats tabgnn only) | 1/2 | Yes (beats tabpfn few) | Yes |

-| none | 7.00 | — | No | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| decoding | 5.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| encoding | 6.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| start | 11.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 4.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+|| columnwise | 1.00 | Yes | Yes | 3/3 | 3/3 | 2/2 | 2/2 | Yes | Yes |

+

+|| none | 7.00 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 2/2 | 2/2 | No | No |

+

+|| decoding | 5.00 | Yes | Yes | 3/3 | 3/3 | 2/2 | 2/2 | No | No |

+

+|| encoding | 6.00 | Yes | Yes | 3/3 | 3/3 | 2/2 | 2/2 | No | No |

+

+|| start | 11.00 | No | No | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 4.00 | Yes | Yes | 3/3 | 3/3 | 2/2 | 2/2 | No | No |

+

 

 註：在這個單一 categorical regression 資料集中，TABTRANSFORMER 的 few-shot `columnwise`、`decoding` 與 `materialize` 表現優異；其中 `columnwise` 更能擊敗多個基線（包含 few-shot 的 tabpfn）。

 

@@ -97,12 +120,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 7.10 | Yes (7.10 < 7.90) | Yes (7.10 < 10.80) | 3/3 | 3/3 | 1/2 (beats tabgnn only) | 1/2 | No | No |

-| none | 7.90 | — | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

-| decoding | 7.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| encoding | 7.80 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| start | 7.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| materialize | 6.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+|| columnwise | 7.10 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| none | 7.90 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 7.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 7.80 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| start | 7.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 6.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：在 large-scale 的數值回歸上，許多 TABTRANSFORMER 的 few-shot 注入能同時擊敗 few-shot 與 full 的 non-GNN 基線，也能勝過所有 few-shot 樹模型；其中 `materialize` 與其他 few-shot GNN 變體尤為強勢，但仍落後於 tabpfn。

 

@@ -121,12 +150,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 14.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 13.93 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 12.71 | Yes (12.71 < 13.93) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 12.93 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 14.93 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 14.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| none | 13.93 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| decoding | 12.71 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 12.93 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| start | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 14.93 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在這些 small balanced binary 任務中，tabtransformer 的 few-shot 注入很少能擊敗全量訓練的樹模型或 tabpfn；僅有少數注入在 few-shot baseline 上帶來小幅改善。

 

@@ -145,12 +180,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 14.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 12.43 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 15.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 14.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 16.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 14.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| none | 12.43 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| decoding | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 15.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| start | 14.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 16.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在這些 small categorical binary 任務中，few-shot tabtransformer 變體未能擊敗強勢的 few/full-sample 基線。

 

@@ -169,12 +210,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 14.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 14.21 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 12.36 | Yes (12.36 < 14.21) | No | 1/3 (beats xgboost few-shot 12.39?) | 0/3 | 1/2 (beats tabgnn few-shot 12.14?) -> careful: 12.36 < 12.14? No -> 0/2 | 0/2 | No | No |

-| encoding | 14.82 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 14.79 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 16.25 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 14.14 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| none | 14.21 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| decoding | 12.36 | Yes | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 14.82 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| start | 14.79 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 16.25 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在這些 small numeric binary 任務中，`decoding` 對 few-shot baseline 有小幅提升，但仍落後於 full-sample 基線與 tabpfn。

 

@@ -193,12 +240,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 11.50 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| none | 6.00 | — | No | 3/3 | 1/3 | 0/2 | 0/2 | Yes (6.00 < 7.50) | No |

-| decoding | 5.83 | Yes (5.83 < 6.00) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes (5.83 < 7.50) | Yes |

-| encoding | 8.17 | No | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 9.67 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| materialize | 5.50 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes | Yes |

+|| columnwise | 11.50 | No | Yes | 3/3 | 1/3 (tie) | 1/2 | 1/2 | No | No |

+

+|| none | 6.00 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

+|| decoding | 5.83 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

+|| encoding | 8.17 | No | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| start | 9.67 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 5.50 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

 

 註：在 small balanced regression 中，若干 few-shot tabtransformer 注入（如 decoding、materialize）能擊敗 few-shot 與 full 非 GNN 基線，並在 few-shot 仍勝過 tabpfn。

 

@@ -216,12 +269,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 7.20 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| none | 5.00 | — | Yes | 3/3 | 3/3 | 0/2 | 0/2 | Yes (5.00 < 6.20) | Yes |

-| decoding | 7.40 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| encoding | 6.20 | Yes (6.20 < 5.00)? No -> careful: 6.20 > 5.00 -> No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 8.20 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| materialize | 7.60 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

+|| columnwise | 7.20 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| none | 5.00 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes | Yes |

+

+|| decoding | 7.40 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| encoding | 6.20 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes (tie) | Yes |

+

+|| start | 8.20 | No | Yes | 2/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| materialize | 7.60 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

 

 註：在 small categorical regression 中，tabgnn 與 tabpfn 佔優；tabtransformer 的 few-shot 變體常能擊敗 full non-GNN 基線，但鮮少超越最強的 few-shot GNN 或 tabpfn。

 

@@ -240,19 +299,25 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 6.44 | Yes (6.44 < 7.19) | Yes | 3/3 | 3/3 | 1/2 (beats t2g? t2g few-shot 15.14 -> yes) | 1/2 | Yes? (6.44 < 5.17 -> No) | No |

-| none | 7.19 | — | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| decoding | 6.97 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

-| encoding | 7.50 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 7.11 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| materialize | 7.78 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

+|| columnwise | 6.44 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| none | 7.19 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| decoding | 6.97 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| encoding | 7.50 | No | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes (tie) |

+

+|| start | 7.11 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| materialize | 7.78 | No | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | No |

+

 

 註：在 small numeric regression 中，few-shot 的 `columnwise` 與 `decoding` 具競爭力，能擊敗多個 few-shot 樹模型，且常能勝過 full non-GNN 基線。

 

 ---

 

-### Takeaways (TABTRANSFORMER)

+### 要點總結 (TABTRANSFORMER)

 - TABTRANSFORMER few-shot GNN injections help most in regression tasks (both large numeric regression and small regression categories). `materialize`, `decoding`, and `columnwise` frequently improve performance vs the few-shot baseline and beat several tree baselines.

 - Classification categories, especially large-dataset classification, are still dominated by fully-sampled `tabpfn` and fully-sampled tree baselines.

 

-(Next: I'll produce the summaries for `trompt` and `vime`.)

+(Next: I'll produce the summaries for `trompt` and `vime`.)
```



## trompt_gnn_enhancement_summary.md

Changed lines (approx): 193



```diff

--- trompt_gnn_enhancement_summary.md.bak

+++ trompt_gnn_enhancement_summary.md

@@ -2,11 +2,16 @@

 

 說明：本檔根據 `trompt_gnn_enhancement.md` 的 avg_rank 值，按 dataset-category 匯總 few-shot（ratio=0.05/0.15/0.8）下各種 GNN 注入變體是否擊敗指定的目標組。

 

-註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。

+註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。

 

 考慮的 few-shot TROMPT 注入：`columnwise`、`none`（few-shot baseline）、`decoding`、`encoding`、`start`、`materialize`（皆為 ratio=0.05/0.15/0.8）。

 

 ---

+

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

 

 ## Category: large_datasets+binclass+numerical (6 datasets)

 (Reference ranks from source)

@@ -22,12 +27,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 9.83 | Yes (9.83 < 11.83) | No | 2/3 | 0/3 | 1/2 (beats tabgnn only) | 1/2 (beats tabgnn full only) | No | No |

-| none (few-shot baseline) | 11.83 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 18.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 10.50 | Yes (10.50 < 11.83) | No | 1/3 (beats xgboost few-shot 12.17) | 0/3 | 1/2 (beats tabgnn only) | 1/2 | No | No |

-| start | 14.17 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 15.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 9.83 | Yes | No | 3/3 (tie) | 0/3 | 1/2 | 1/2 | No | No |

+

+|| none (few-shot baseline) | 11.83 | No (tie, few-shot strict) | No | 1/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 18.00 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| encoding | 10.50 | Yes | No | 2/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| start | 14.17 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 15.67 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

 

 註：在 large binary numeric 任務中，TROMPT 的 few-shot `columnwise` 與 `encoding` 較 few-shot baseline 有所改善，但尚未有任何注入能擊敗 full-sample 的 TROMPT 或最強的 full-sample 樹模型與 tabpfn。

 

@@ -47,12 +58,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 12.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 11.00 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 19.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 9.67 | Yes (9.67 < 11.00) | No | 1/3 | 0/3 | 1/2 | 0/2 | Yes (9.67 < 10.33) | No |

-| start | 16.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 17.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 12.33 | No | No | 1/3 (tie) | 0/3 | 1/2 | 1/2 (tie) | No | No |

+

+|| none | 11.00 | No (tie, few-shot strict) | No | 2/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 19.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 9.67 | Yes | No | 3/3 | 0/3 | 1/2 | 1/2 | Yes | No |

+

+|| start | 16.67 | No | No | 0/3 | 0/3 | 1/2 (tie) | 0/2 | No | No |

+

+|| materialize | 17.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 large multiclass 資料集中，TROMPT 的 few-shot `encoding` 是唯一相較於 few-shot baseline 有改善的注入。

 

@@ -72,12 +89,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| decoding | 1.00 | Yes (1.00 < 13.00) | Yes (1.00 < 19.00) | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 | No | No |

-| none | 13.00 | — | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| materialize | 6.00 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| encoding | 12.00 | Yes (12.00 < 13.00) | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| columnwise | 14.00 | No | Yes | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 9.00 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+|| decoding | 1.00 | Yes | Yes | 3/3 | 3/3 | 2/2 | 2/2 | Yes | Yes |

+

+|| none | 13.00 | No (tie, few-shot strict) | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 6.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 12.00 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+

+|| columnwise | 14.00 | No | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+

+|| start | 9.00 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

+

 

 註：在這個 categorical regression 單一資料集中，TROMPT 的 few-shot `decoding` 表現卓越，幾乎擊敗所有參考模型。

 

@@ -97,12 +120,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| decoding | 3.20 | Yes (3.20 < 10.80) | Yes (3.20 < 17.60) | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 | Yes (3.20 < 1.80? No) -> correction: 3.20 > 1.80, so No | No |

-| materialize | 6.90 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| start | 7.10 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| encoding | 10.40 | Yes (10.40 < 10.80) | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| columnwise | 10.70 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| none | 10.80 | — | No | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

+|| decoding | 3.20 | Yes | Yes | 3/3 | 3/3 | 2/2 | 2/2 | No | No |

+

+|| materialize | 6.90 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| start | 7.10 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 10.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| columnwise | 10.70 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| none | 10.80 | No (tie, few-shot strict) | Yes | 3/3 (tie) | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：在 large numeric regression 中，TROMPT 的 few-shot `decoding`、`materialize`、`start` 強力擊敗了 few-shot 與 full 的 non-GNN 基線，並且擊敗所有 few-shot 樹模型。

 

@@ -121,12 +150,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 11.79 | No | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 10.86 | — | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 13.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 12.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 13.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 15.07 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 11.79 | No | No | 1/3 (tie) | 0/3 | 1/2 | 1/2 (tie) | No | No |

+

+|| none | 10.86 | No (tie, few-shot strict) | No | 3/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 13.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 12.71 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| start | 13.29 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 15.07 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 small balanced binary 資料集中，few-shot 的 TROMPT 注入通常無法擊敗最強的 few/full-sample 基線。

 

@@ -145,12 +180,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 8.43 | No | Yes | 1/3 | 2/3 | 0/2 | 0/2 | No | Yes |

-| none | 7.43 | — | Yes | 2/3 | 2/3 | 1/2 | 1/2 | No | Yes |

-| decoding | 15.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 8.00 | No (8.00 > 7.43) | Yes | 1/3 | 2/3 | 0/2 | 0/2 | No | Yes |

-| start | 12.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 15.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 8.43 | No | No | 2/3 | 3/3 | 2/2 | 1/2 | Yes | Yes |

+

+|| none | 7.43 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 2/2 | 1/2 | Yes | Yes |

+

+|| decoding | 15.71 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 8.00 | No | Yes (tie) | 3/3 (tie) | 3/3 | 2/2 | 1/2 | Yes | Yes |

+

+|| start | 12.00 | No | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| materialize | 15.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 small categorical binary 中結果混合，full-sample 基線仍然強勢。

 

@@ -169,12 +210,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 11.43 | Yes (11.43 < 11.96) | No | 1/3 | 0/3 | 1/2? (11.43 < 11.64 -> yes beating tabgnn few) | 0/2 | No | No |

-| none | 11.96 | — | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 10.00 | Yes | No | 2/3 | 0/3 | 1/2 | 0/2 | Yes (10.00 < 10.61) | No |

-| start | 13.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 13.36 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 11.43 | Yes | No | 2/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| none | 11.96 | No (tie, few-shot strict) | No | 2/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| decoding | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 10.00 | Yes | No | 3/3 | 0/3 | 2/2 | 1/2 | Yes | No |

+

+|| start | 13.00 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| materialize | 13.36 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

 

 註：在 small numeric binary 任務中，少數注入（如 `encoding`）可較 few-shot baseline 改善並擊敗一些 few-shot 樹模型或 tabpfn，但 full-sample 基線仍占優。

 

@@ -193,12 +240,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 12.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 9.50 | — | No | 2/3 | 0/3 | 1/2 | 1/2 | No | No |

-| decoding | 5.33 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes (5.33 < 7.67) | Yes |

-| encoding | 10.00 | No | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 7.83 | Yes (7.83 < 9.50) | Yes | 2/3 | 2/3 | 0/2 | 0/2 | Yes (7.83 < 7.67)? No -> 7.83 > 7.67 so No | No |

-| materialize | 9.17 | No | Yes | 1/3 | 1/3 | 0/2 | 0/2 | No | Yes |

+|| columnwise | 12.50 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| none | 9.50 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 5.33 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

+|| encoding | 10.00 | No | Yes | 3/3 | 2/3 | 1/2 | 1/2 | No | No |

+

+|| start | 7.83 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| materialize | 9.17 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：在 small balanced regression 中，TROMPT 的 `decoding` 表現突出，能擊敗 few-shot 與 full 的 non-GNN 基線，並勝過 few/full 的 tabpfn。

 

@@ -216,12 +269,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 10.20 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| none | 8.20 | — | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No (8.20 < 8.60) | Yes |

-| decoding | 4.80 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes (4.80 < 8.60) | Yes |

-| encoding | 10.40 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 6.00 | Yes | Yes | 2/3 | 3/3 | 0/2 | 0/2 | Yes (6.00 < 8.60) | Yes |

-| materialize | 5.40 | Yes | Yes | 2/3 | 3/3 | 0/2 | 0/2 | Yes | Yes |

+|| columnwise | 10.20 | No | Yes | 0/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| none | 8.20 | No (tie, few-shot strict) | Yes | 2/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| decoding | 4.80 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| encoding | 10.40 | No | Yes | 0/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| start | 6.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| materialize | 5.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

 

 註：在 small categorical regression 中，數個 trompt 的 few-shot 注入（decoding/materialize/start）能擊敗多項參考方法，包含 tabpfn。

 

@@ -240,19 +299,25 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 10.50 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| none | 9.14 | — | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| decoding | 3.89 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes (3.89 < 4.39) | Yes |

-| encoding | 9.36 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 8.17 | Yes (8.17 < 9.14) | Yes | 2/3 | 3/3 | 0/2 | 0/2 | Yes? (8.17 < 4.39? No) | Yes |

-| materialize | 8.72 | Yes | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

+|| columnwise | 10.50 | No | Yes | 0/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| none | 9.14 | No (tie, few-shot strict) | Yes | 1/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 3.89 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | Yes | Yes |

+

+|| encoding | 9.36 | No | Yes | 1/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| start | 8.17 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | No |

+

+|| materialize | 8.72 | Yes | Yes | 2/3 (tie) | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：TROMPT 的 few-shot `decoding` 對小型數值回歸特別有效，能擊敗 few-shot 的 tabpfn 與多數參考模型。

 

 ---

 

-### Takeaways (TROMPT)

+### 要點總結 (TROMPT)

 - TROMPT few-shot GNN injections (notably `decoding`, `materialize`, `start`) provide large benefits in several regression categories, often beating few-shot trees and even tabpfn in small regression settings.

 - For classification tasks, fully-sampled baselines still commonly win.

 

-(Next: I'll create the `vime` summary.)

+(Next: I'll create the `vime` summary.)
```



## vime_gnn_enhancement_summary.md

Changed lines (approx): 193



```diff

--- vime_gnn_enhancement_summary.md.bak

+++ vime_gnn_enhancement_summary.md

@@ -2,11 +2,16 @@

 

 說明：本檔根據 `vime_gnn_enhancement.md` 的 avg_rank 值，按 dataset-category 匯總 few-shot（ratio=0.05/0.15/0.8）下各種 GNN 注入變體是否擊敗指定的目標組。

 

-註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie)" 標示；所有標註為 (tie) 的情況皆已視為 beats。

+註：在此更新中，針對「beats few-shot 原始模型」（few-shot non-GNN baseline）仍採用嚴格比較（avg_rank 更小才算為勝）。對於其它比較（對樹模型、GNN、tabpfn 等群組），若 avg_rank 相等則視為擊敗，且在表格中以 " (tie, few-shot strict)" 標示；所有標註為 (tie, few-shot strict) 的情況皆已視為 beats。

 

 考慮的 few-shot VIME 注入：`columnwise`、`none`（few-shot baseline）、`decoding`、`encoding`、`start`、`materialize`（皆為 ratio=0.05/0.15/0.8）。

 

 ---

+

+註：本檔使用容差 1e-3 判斷 avg_rank 是否相等。

+- 對於「beats few-shot 原始模型（few-shot non-GNN，ratio=0.05/0.15/0.8, gnn_stage=none）」的判定，使用嚴格比較（必須 strictly lower 才算 beats）。

+- 對於其它比較（樹模型、GNN、tabpfn 等），若 avg_rank 在容差範圍內視為平手，並在表格中以 'Yes (tie)' 標示；若平手但涉及 few-shot 原始模型則標示 'No (tie, few-shot strict)'。

+

 

 ## Category: large_datasets+binclass+numerical (6 datasets)

 (Reference ranks from source)

@@ -22,12 +27,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 13.00 | Yes (13.00 < 13.17) | No | 0/3 | 0/3 | 1/2 (beats tabgnn only) | 1/2 | No | No |

-| none (few-shot baseline) | 13.17 | — | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

-| decoding | 15.50 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 13.00 | Yes (13.00 < 13.17) | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

-| start | 16.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 15.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 13.00 | Yes | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| none (few-shot baseline) | 13.17 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 15.50 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 13.00 | Yes | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| start | 16.67 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 15.83 | No | No | 0/3 | 0/3 | 1/2 | 1/2 | No | No |

+

 

 註：VIME 的 few-shot `columnwise` 與 `encoding` 對 few-shot baseline 有小幅提升，但在此類別中尚未超越 full-sample 基線。

 

@@ -47,12 +58,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 15.33 | Yes (15.33 < 15.67) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| none | 15.67 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 14.00 | Yes (14.00 < 15.67) | No | 0/3 | 0/3 | 1/2 | 0/2 | No | No |

-| start | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 17.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 15.33 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| none | 15.67 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| decoding | 17.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 14.00 | Yes | No | 0/3 | 0/3 | 1/2 (tie) | 0/2 | No | No |

+

+|| start | 18.33 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 17.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 small 類別中，`encoding`/`columnwise` 對 few-shot baseline 有小幅增益，但無法與 full-sample 的領先者競爭。

 

@@ -72,12 +89,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| decoding | 5.00 | Yes (5.00 < 10.00) | Yes (5.00 < 13.00) | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 | No | No |

-| none | 10.00 | — | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| materialize | 15.83 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 9.00 | Yes (9.00 < 10.00) | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| columnwise | 8.00 | Yes | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | No |

-| start | 16.67 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| decoding | 5.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| none | 10.00 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 15.83 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 9.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| columnwise | 8.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| start | 16.67 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：在這個 categorical regression 的單一資料集中，VIME 的 few-shot `decoding` 與 `encoding` 能擊敗多項參考基線（包含 full-sample non-GNN）。

 

@@ -97,12 +120,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree (out of 3) | beats full tree (out of 3) | beats few-shot GNN (out of 2) | beats full GNN (out of 2) | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 9.20 | No | Yes | 3/3 | 3/3 | 1/2 (beats t2g only) | 1/2 | No | No |

-| none | 8.30 | — | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

-| decoding | 8.50 | No | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

-| encoding | 8.20 | No | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | No |

-| start | 5.70 | Yes (5.70 < 8.30) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

-| materialize | 6.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+|| columnwise | 9.20 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| none | 8.30 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 8.50 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 8.20 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| start | 5.70 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 6.00 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：在 large-scale 的數值回歸中，VIME 的 few-shot `start` 與 `materialize` 有顯著幫助，能擊敗 few-shot 與 full 的 non-GNN 基線，並勝過所有 few-shot 樹模型。

 

@@ -121,12 +150,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 11.36 | Yes (11.36 < 11.50) | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

-| none | 11.50 | — | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 12.00 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 11.79 | Yes (11.79 < 11.50)? No -> 11.79 > 11.50 -> No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| start | 15.64 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 11.36 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| none | 11.50 | No (tie, few-shot strict) | No | 2/3 (tie) | 0/3 | 1/2 | 0/2 | No | No |

+

+|| decoding | 12.00 | No | No | 1/3 (tie) | 0/3 | 1/2 | 0/2 | No | No |

+

+|| encoding | 11.79 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| start | 15.64 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 14.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 small balanced binary 任務中，VIME 的 few-shot 注入僅帶來些微變化，沒有擊敗最強的 full-sample 基線。

 

@@ -145,12 +180,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 11.36 | Yes (11.36 < 12.57) | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

-| none | 12.57 | — | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 13.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 11.57 | Yes (11.57 < 12.57) | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

-| start | 14.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 17.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 11.36 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| none | 12.57 | No (tie, few-shot strict) | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| decoding | 13.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| encoding | 11.57 | Yes | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| start | 14.14 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 17.86 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 small categorical binary 中，VIME 的 few-shot `columnwise`/`encoding` 稍微優於 few-shot baseline，但仍無法挑戰 full-sample 的領先者。

 

@@ -169,12 +210,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 11.36 | — (equal to few-shot none=11.36) | No | 1/3 | 0/3 | 1/2 (beats tabgnn few 14.39) | 0/2 | No | No |

-| none | 11.36 | — | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

-| decoding | 14.36 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| encoding | 10.57 | Yes (10.57 < 11.36) | No | 2/3 | 0/3 | 1/2 | 0/2 | Yes (10.57 < 10.82) | No |

-| start | 14.57 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

-| materialize | 14.79 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+|| columnwise | 11.36 | No (tie, few-shot strict) | No | 3/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| none | 11.36 | No (tie, few-shot strict) | No | 3/3 | 0/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 14.36 | No | No | 1/3 | 0/3 | 1/2 | 0/2 | No | No |

+

+|| encoding | 10.57 | Yes | No | 3/3 | 0/3 | 1/2 | 1/2 | Yes | No |

+

+|| start | 14.57 | No | No | 1/3 | 0/3 | 0/2 | 0/2 | No | No |

+

+|| materialize | 14.79 | No | No | 0/3 | 0/3 | 0/2 | 0/2 | No | No |

+

 

 註：在 small numeric binary 中，部分 few-shot 注入（如 `encoding`）對 few-shot baseline 有小幅改善，但 full-sample 基線仍然居前。

 

@@ -193,12 +240,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 10.17 | No | Yes | 2/3 | 2/3 | 0/2 | 0/2 | No | Yes |

-| none | 10.00 | — | Yes | 2/3 | 1/3 | 0/2 | 0/2 | No | Yes |

-| decoding | 8.67 | Yes (8.67 < 10.00) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | Yes (8.67 < 7.00)? No -> 8.67 > 7.00 -> No | Yes |

-| encoding | 9.67 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 8.33 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | Yes? (8.33 < 7.00? No) | Yes |

-| materialize | 8.33 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

+|| columnwise | 10.17 | No | Yes | 3/3 | 2/3 | 1/2 | 1/2 | No | No |

+

+|| none | 10.00 | No (tie, few-shot strict) | Yes | 3/3 | 2/3 | 1/2 | 1/2 | No | No |

+

+|| decoding | 8.67 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| encoding | 9.67 | Yes | Yes | 3/3 | 3/3 (tie) | 1/2 | 1/2 | No | No |

+

+|| start | 8.33 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

+|| materialize | 8.33 | Yes | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | No |

+

 

 註：在 small balanced regression 中，VIME 的 few-shot 注入（decoding/encoding/start/materialize）普遍能擊敗 full 非 GNN 基線與多數樹模型。

 

@@ -216,12 +269,18 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 7.00 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| none | 6.60 | — | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No (6.60 < 3.80? No) | Yes |

-| decoding | 5.40 | Yes (5.40 < 6.60) | Yes | 2/3 | 3/3 | 0/2 | 0/2 | Yes (5.40 < 3.80? No) | Yes |

-| encoding | 6.60 | — (tie) | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| start | 7.20 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| materialize | 7.80 | No | Yes | 1/3 | 3/3 | 0/2 | 0/2 | No | Yes |

+|| columnwise | 7.00 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| none | 6.60 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| decoding | 5.40 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| encoding | 6.60 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| start | 7.20 | No | Yes | 3/3 | 3/3 | 1/2 | 1/2 | No | Yes |

+

+|| materialize | 7.80 | No | Yes | 3/3 (tie) | 3/3 | 1/2 | 1/2 | No | Yes |

+

 

 註：在 small categorical regression 中，VIME 的 `decoding` 為最強的 few-shot 注入，能擊敗多數參考方法。

 

@@ -240,19 +299,25 @@

 

 | Injection | avg_rank | beats few-shot-non-gnn? | beats full-non-gnn? | beats few-shot tree | beats full tree | beats few-shot GNN | beats full GNN | beats few-shot tabpfn? | beats full tabpfn? |

 |---|---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

-| columnwise | 7.50 | Yes (7.50 < 7.61) | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No (7.50 < 3.64? No) | Yes |

-| none | 7.61 | — | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| decoding | 8.83 | No | Yes | 2/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| encoding | 6.92 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | Yes? (6.92 < 3.64? No) | Yes |

-| start | 6.89 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | No | Yes |

-| materialize | 6.08 | Yes | Yes | 3/3 | 3/3 | 0/2 | 0/2 | Yes (6.08 < 3.64? No) | Yes |

+|| columnwise | 7.50 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| none | 7.61 | No (tie, few-shot strict) | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| decoding | 8.83 | No | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | No |

+

+|| encoding | 6.92 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| start | 6.89 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

+|| materialize | 6.08 | Yes | Yes | 3/3 | 3/3 | 1/2 | 2/2 | No | Yes |

+

 

 註：VIME 的 few-shot `materialize`/`start`/`encoding` 在 small numeric regression 中有幫助，能擊敗多個樹模型基線與 full 非 GNN 基線。

 

 ---

 

-### Takeaways (VIME)

+### 要點總結 (VIME)

 - VIME few-shot GNN injections provide consistent gains in regression tasks (both large and small). `start`, `materialize`, and `decoding` are often the most helpful stages depending on dataset type.

 - In classification categories, fully-sampled `tabpfn` and fully-sampled tree baselines often remain dominant.

 

-(End of VIME summary.)

+(End of VIME summary.)
```


